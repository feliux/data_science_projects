{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "\n",
    "- [Machine Learning con Spark](#ml)\n",
    "\n",
    "    - [Spark.ml](#spark.ml)\n",
    "        - [Tipos de datos](#data)\n",
    "        - [Extractores, transformadores y selectores](#data2)\n",
    "        \n",
    "    - [Aprendizaje supervisado](#supervisado)\n",
    "        - [Regresión](#regresion)\n",
    "            - [Linear Regression](#lr)\n",
    "            - [Survival Regression](#sr)\n",
    "            - [Decision Trees](#dt)\n",
    "            - [Random Forest](#rf)\n",
    "            - [Gradient Boosted Trees](#gbt)\n",
    "            - [Isotonic Regression](#riso)\n",
    "        - [Clasificación](#clasificacion)\n",
    "            - [Linear Regression](#lrb)\n",
    "            - [Decision Trees](#dtc)\n",
    "            - [Gradient Boosted Trees](#gbtc)\n",
    "            - [Random Forest](#rfc)\n",
    "            - [Naive Bayes](#nb)\n",
    "            - [Perceptron](#perceptron)\n",
    "            \n",
    "        - [Cross Validation](#cv)\n",
    "        - [Spark Pipeline](#pipeline)\n",
    "        \n",
    "    - [Spark.mllib](#mllib)\n",
    "    \n",
    "<div id='xx' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='ml' />\n",
    "\n",
    "# Machine Learning con Spark\n",
    "\n",
    "MLlib es la librería de Spark de aprendizaje automático. Su objetivo es hacer machine learning escalable y sencillo.\n",
    "\n",
    "Se compone de algortimos comunes de aprendizaje y utilidades, incluyendo clasificación, regresión, clustering, filtros colaborativos, reducción de dimensionalidad asó como de primtivas de optimización de bajo nivel y APIs de flujo de alto nivel.\n",
    "\n",
    "Está formado por dos paquetes:\n",
    "\n",
    "- ***Spark.mllib***, que contiene el API original sobre RDDs.\n",
    "\n",
    "- ***Spark.ml***, proporciona APIs de más alto nivel contruidas sobre dataframes.\n",
    "\n",
    "<div id='spark.ml' />\n",
    "\n",
    "## Spark.ml\n",
    "\n",
    "Los conceptos claves introducidos por el API de [Spark.ml](https://spark.apache.org/docs/latest/ml-guide.html) son los siguientes:\n",
    "\n",
    "- **Dataframe**: Spark ML usa los dataframes de Spark SQL como los dataset principales para tareas de ML.\n",
    "\n",
    "- **Transformer**: se trata de un algoritmo que puede transformar un dataframe en otro dataframe, por ejemplo, un modelo de ML es un *Transformer* que transforma un dataframe con variables predictoras en otro con predicciones.\n",
    "\n",
    "- **Estimator**: es un algoritmo que se puede aplicar sobre un dataframe para producir un *Transformer*. Por ejemplo, un algoritmo de aprendizaje es un *Estimator* que se entrena con un dataframe y produce un modelo.\n",
    "\n",
    "- **Pipeline**: encadena múltiples *Transformes* y *Estimators* juntos especificando un flujo de trabajo ML.\n",
    "\n",
    "- **Parameter**: todos los *Transformer* y *Estimators* comparten ahora un API para especificar los parámetros.\n",
    "\n",
    "Como salida a la aplicación de cada uno de los algoritmos se obtiene un objeto tipo *model* de la clase correspondiente. Por ejemplo, si aplicamos LinearRegresion obtenemos LineaRegresionModel.\n",
    "\n",
    "<div id='data' />\n",
    "\n",
    "### Tipos de datos especiales\n",
    "\n",
    "Las librerías ml y mllib soportan los siguientes tipos de datos especiales:\n",
    "\n",
    "- **Vectores locales** que pueden ser:\n",
    "\n",
    "    - **Vectores densos (dense)** en cuyo caso se especifican todos los valores del vector y se pueden crear a partir de arrays numpy y listas python.\n",
    "    \n",
    "    - **Vectores dispersos (sparse)** aquellos que contienen una gran cantidad de ceros, por lo que se definen especificando únicamente los valores no nulos junto con sus posiciones. Se pueden crear a partir de la clase *SparseVector* de mllib y *csc_matrix* de scipy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.0,0.0,5.5]\n",
      "(4,[1,3],[1.0,5.5])\n",
      "(4,[1,3],[1.0,5.5])\n",
      "(4,[1,3],[1.0,5.5])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Distintas formas de crear un mismo vector.\n",
    "\n",
    "vdense = Vectors.dense([0, 1.0, 0, 5.5])\n",
    "print(vdense)\n",
    "\n",
    "# Vecotors.sparse se indica la longitud y posteriormente las posiciones con sus respectivos valores.\n",
    "\n",
    "vsparse1 = Vectors.sparse(4, {1: 1.0, 3: 5.5})\n",
    "print(vsparse1)\n",
    "\n",
    "vsparse2 = Vectors.sparse(4, [(1, 1.0), (3, 5.5)])\n",
    "print(vsparse2)\n",
    "\n",
    "vsparse3 = Vectors.sparse(4, [1, 3], [1.0, 5.5])\n",
    "print(vsparse3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la manipulación de vectores se recomienda utilizar los comandos implementados en la clase *Vectors*:\n",
    "\n",
    "- **Labeled Points**: vectores locales (densos o dispersos) que tienen asociados una respuesta o etiqueta numérica que a menudo suelen representar una predicción. Pueden ser de los siguientes tipos:\n",
    "    \n",
    "    - **Double** en casos de regresión.\n",
    "    \n",
    "    - **0/1** en casos de clasificación binaria.\n",
    "      \n",
    "    - **Índices numéricos** en caso de clasificación multiclase.\n",
    "    \n",
    "Los vectores se utilizan normalmente con la librería spark.ml mientras que los puntos etiquetados suelen ser más utilizados con spark.mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[1.0,0.0,3.0])\n",
      "(1.0,(3,[0,1],[2.0,3.0]))\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Creación a partir de un dense vector.\n",
    "\n",
    "ld = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "print(ld)\n",
    "\n",
    "# Creación a partir de un sparse vector.\n",
    "\n",
    "ls = LabeledPoint(1.0, Vectors.sparse(3, [0, 1.0], [2, 3.0]))\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de estos dos tipos principales existen las **Matrices Locales** densas y/o dispersas y las **Matrices Distribuidas** que a su vez se diferencian en:\n",
    "\n",
    "    - Row Matrix\n",
    "    - IndexedRowMatrix\n",
    "    - CoordinateMatrix\n",
    "    - Block Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='data2' />\n",
    "\n",
    "#### Preparación de datos: extractores, transformadores y selectores\n",
    "\n",
    "Existen distintos componentes que permiten realizar múltiples tareas sobre los dataframe que ayudan a preparar los datos. Hay tres tipos: extractores, transformadores, selectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "| ID|sepal_length|sepal_width|petal_length|petal_width|     specie|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Tipado de datos: DoubleType\n",
    "\n",
    "fields = [StructField(\"ID\", IntegerType(), True), StructField(\"sepal_length\", DoubleType(), True), StructField(\"sepal_width\", DoubleType(), True), StructField(\"petal_length\", DoubleType(), True), StructField(\"petal_width\", DoubleType(), True), StructField(\"specie\", StringType(), True)]\n",
    "schema = StructType(fields)\n",
    "\n",
    "iris_csvDF = spark.read.format(\"csv\").options(header = True).load(\"../data/iris.csv\", schema = schema)\n",
    "iris_csvDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+-----------------+\n",
      "| ID|sepal_length|sepal_width|petal_length|petal_width|     specie|         features|\n",
      "+---+------------+-----------+------------+-----------+-----------+-----------------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "+---+------------+-----------+------------+-----------+-----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler as VA\n",
    "\n",
    "# Construimos un assmebler para unir todas las columnas predictoras del dataset en un vector de features.\n",
    "\n",
    "v = VA(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
    "d = v.transform(iris_csvDF)\n",
    "d.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='supervisado' />\n",
    "\n",
    "## Aprendizaje supervisado\n",
    "\n",
    "<div id='regresion' />\n",
    "\n",
    "### Regresión\n",
    "\n",
    "Spark ML implementa varios algoritmos para realizar regresiones:\n",
    "\n",
    "<div id='lr' />\n",
    "\n",
    "#### Regresión lineal (Linear Regression)\n",
    "\n",
    "`LinearRegression?` soporta múltiples tipos de regularización dependiendo del valor del parámetro `elasticNetParam` y `regParam`:\n",
    "\n",
    "- **Ordinary Least Squares (OLS)** (ninguna regularización): `regParam = 0`\n",
    "    \n",
    "- **L2 (ridge regression)**: `elasticNetParam = 0, regParam > 0`\n",
    "\n",
    "- **L1 (lasso)**: `elasticNetParam = 1, regParam > 0`\n",
    "\n",
    "- **L2+L1 (elastic net)**: `0 < elasticNetParam < 1, regParam > 0`\n",
    "\n",
    "<div id='sr' />\n",
    "\n",
    "#### Regresión de supervivencia (Survival Regression)\n",
    "\n",
    "La clase spark que utilizamos para este tipo de modelos es `AFTSurvivalRegression?`. Esta clase implementa el modelo [*AFT (Accelerate Failure Time)*](https://en.wikipedia.org/wiki/Accelerated_failure_time_model) basado en la distribución de Weibull de tiempo de supervivencia.\n",
    "\n",
    "<div id='dt' />\n",
    "\n",
    "#### Regresión con árboles de desición (Decision Trees)\n",
    "\n",
    "Este modelo viene con la clase `DecisionTreeRegressor?`. Soporta variables predictoras de tipo *continuo* o *categóricas*.\n",
    "\n",
    "<div id='rf' />\n",
    "\n",
    "#### Regresión con Random Forest\n",
    "\n",
    "`RandomForestRegressor?` es la clase de spark para este tipo de algoritmos. Al igual que Dessicion Tree también soporta variables predictoras de tipo *continuo* o *categórica*.\n",
    "\n",
    "<div id='gbt' />\n",
    "\n",
    "#### Regresión GBT (Gradient Boosted Trees)\n",
    "\n",
    "Este modelo se implementa en la clase `GBTRegressor?`. Soporta variables predictoras de tipo *continuo* o *categórica*.\n",
    "\n",
    "<div id='riso' />\n",
    "\n",
    "#### Regresión isotónica\n",
    "\n",
    "La clase spark correspondiente es `IsotonicRegression?`. [*Isotonic Regression*](https://en.wikipedia.org/wiki/Isotonic_regression) se encuentra en fase experimental en spark.ml y sólo admite una única variable predictora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+--------+\n",
      "|_c0|eruptions|waiting|features|\n",
      "+---+---------+-------+--------+\n",
      "|  1|      3.6|     79|  [79.0]|\n",
      "|  2|      1.8|     54|  [54.0]|\n",
      "|  3|    3.333|     74|  [74.0]|\n",
      "+---+---------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Intercept: -1.9156822121843768\n",
      "Coefficients: [0.07661202567343066]\n",
      " \n",
      "+---+---------+-------+--------+------------------+\n",
      "|_c0|eruptions|waiting|features|        prediction|\n",
      "+---+---------+-------+--------+------------------+\n",
      "|  1|      3.6|     79|  [79.0]| 4.136667816016645|\n",
      "|  2|      1.8|     54|  [54.0]|2.2213671741808785|\n",
      "|  3|    3.333|     74|  [74.0]|3.7536076876494917|\n",
      "| 14|     1.75|     47|  [47.0]|1.6850829944668642|\n",
      "| 27|    1.967|     55|  [55.0]| 2.297979199854309|\n",
      "+---+---------+-------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "RMSE test: 0.45981889015866745\n",
      "RMSE training: 0.5079462531939202\n",
      "RMSE training: 0.507946\n",
      "R2 training: 0.796555\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.feature import VectorAssembler as VA\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import IsotonicRegression\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Carga de datos.\n",
    "\n",
    "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"../data/faithful.csv\")\n",
    "\n",
    "# Creamos un VectorAssembler (vector de características) para tener todas las variables predictoras en un vector features.\n",
    "\n",
    "vAssem = VA(inputCols=[\"waiting\"], outputCol=\"features\")\n",
    "vt = vAssem.transform(df)\n",
    "vt.show(3)\n",
    "\n",
    "# Creamos conjunto entrenamiento y test.\n",
    "\n",
    "splits = vt.randomSplit([0.7, 0.3], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Definimos los algoritmos de regresión indicando la columna target=eruptions.\n",
    "# Aplicamos el modelo sobre el conjunto de entrenamiento.\n",
    "\n",
    "lr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\", labelCol=\"eruptions\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "#aft = AFTSurvivalRegression(maxIter=5, labelCol=\"eruptions\", featuresCol=\"features\")\n",
    "#aft_model = aft.fit(train)\n",
    "\n",
    "dt = DecisionTreeRegressor(maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0, labelCol=\"eruptions\", featuresCol=\"features\", checkpointInterval=10)\n",
    "dt_model = dt.fit(train)\n",
    "\n",
    "rf = RandomForestRegressor(maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0, labelCol=\"eruptions\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "gbt = GBTRegressor(maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0, labelCol=\"eruptions\", featuresCol=\"features\", checkpointInterval=10, lossType=\"squared\")\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "ir = IsotonicRegression(labelCol=\"eruptions\", featuresCol=\"features\", isotonic=True)\n",
    "ir_model = ir.fit(train)\n",
    "\n",
    "# Obtenemos algunos parámetros del modelo.\n",
    "\n",
    "print(\"Intercept: {0}\".format(lr_model.intercept))\n",
    "print(\"Coefficients: {0}\".format(lr_model.coefficients))\n",
    "print(\" \")\n",
    "\n",
    "# Hacemos predicciones sobre el conjunto de test.\n",
    "\n",
    "predict_train = lr_model.transform(train)\n",
    "predict_test = lr_model.transform(test)\n",
    "predict_test.show(5)\n",
    "\n",
    "# Usamos un evaluador para conocer el error de entrenamiento y test.\n",
    "\n",
    "ev = RegressionEvaluator(labelCol=\"eruptions\")\n",
    "print(\"RMSE test: {0}\".format(ev.evaluate(predict_test, {ev.metricName: \"rmse\"})))\n",
    "print(\"RMSE training: {0}\".format(ev.evaluate(predict_train, {ev.metricName: \"rmse\"})))\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE training: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R2 training: %f\" % trainingSummary.r2)\n",
    "\n",
    "# Lista de parámetros\n",
    "\n",
    "#lr_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='clasificacion' />\n",
    "\n",
    "### Clasificación\n",
    "\n",
    "<div id='lrb' />\n",
    "\n",
    "#### Clasificación mediante regresión logística (binaria)\n",
    "\n",
    "`LogisticRegression?` sólo permite clasificaciones binarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----+-------+-------------+--------------------+\n",
      "|admit|  gre| gpa|rank|ranknum|   dummy_rank|            features|\n",
      "+-----+-----+----+----+-------+-------------+--------------------+\n",
      "|  0.0|380.0|3.61|   3|    1.0|(4,[1],[1.0])|[380.0,3.61,0.0,1...|\n",
      "|  1.0|660.0|3.67|   3|    1.0|(4,[1],[1.0])|[660.0,3.67,0.0,1...|\n",
      "|  1.0|800.0| 4.0|   1|    3.0|(4,[3],[1.0])|[800.0,4.0,0.0,0....|\n",
      "+-----+-----+----+----+-------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Intercept: -0.8943498051582796\n",
      "Coefficients: [0.0017865683210316042,-0.24545377013623773,0.0440126119321804,-0.5289020158341485,-0.7855737230162501,0.8250528991613559]\n",
      " \n",
      "+-----+-----+----+----+-------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|admit|  gre| gpa|rank|ranknum|   dummy_rank|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+-----+----+----+-------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|220.0|2.83|   3|    1.0|(4,[1],[1.0])|[220.0,2.83,0.0,1...|[1.72484095985102...|[0.84875133224047...|       0.0|\n",
      "|  0.0|300.0|2.92|   4|    2.0|(4,[2],[1.0])|[300.0,2.92,0.0,0...|[1.86067804066286...|[0.86537595963320...|       0.0|\n",
      "|  0.0|300.0|3.01|   3|    1.0|(4,[1],[1.0])|[300.0,3.01,0.0,1...|[1.62609717279302...|[0.83563428887320...|       0.0|\n",
      "|  0.0|380.0|3.34|   3|    1.0|(4,[1],[1.0])|[380.0,3.34,0.0,1...|[1.56417145125545...|[0.82695111443624...|       0.0|\n",
      "|  0.0|400.0|3.65|   2|    0.0|(4,[0],[1.0])|[400.0,3.65,1.0,0...|[1.03161612581072...|[0.73722909554354...|       0.0|\n",
      "+-----+-----+----+----+-------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Área bajo curva ROC: 0.6750380517503805\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.feature import VectorAssembler as VA\n",
    "#from pyspark.sql.typer import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Nos interesa predecir la variable admit, la cual nos dice si un alumno\n",
    "# es admitido en una escuela superior. Las otras variables son:\n",
    "# gre (Graduate Record Exam scores)\n",
    "# gpa (Grade Point Average)\n",
    "# rank: prestigio de la escuela de donde viene el alumno.\n",
    "\n",
    "# Nos aseguramos de que el target sea DoubleType aunque el modelo también funciona si definimos este campo de tipo Binary o Boolean\n",
    "\n",
    "ad_schema = StructType([StructField(\"admit\", DoubleType(), True),\n",
    "                        StructField(\"gre\", DoubleType(), True), \n",
    "                        StructField(\"gpa\", DoubleType(), True),\n",
    "                        StructField(\"rank\", StringType(), True)])\n",
    "\n",
    "data = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"../data/binary.csv\", schema=ad_schema)\n",
    "\n",
    "# Transformamos la variable rank en una lista de valores numéricos.\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"rank\", outputCol=\"ranknum\")\n",
    "i_model = indexer.fit(data)\n",
    "i_data = i_model.transform(data)\n",
    "\n",
    "# Transformamos la columna con la variable categórica ranknum en variables dummies.\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"ranknum\", outputCol=\"dummy_rank\", dropLast=False)\n",
    "dum_data = encoder.transform(i_data)\n",
    "\n",
    "# Construimos el assembler para unir todas las columnas predictoras en un vector features.\n",
    "\n",
    "vAssem2 = VA(inputCols=[\"gre\", \"gpa\", \"dummy_rank\"], outputCol=\"features\")\n",
    "vt2 = vAssem2.transform(dum_data)\n",
    "vt2.show(3)\n",
    "\n",
    "# Creamos conjunto entrenamiento y test.\n",
    "\n",
    "splits2 = vt2.randomSplit([0.7, 0.3], 1234)\n",
    "train2 = splits2[0]\n",
    "test2 = splits2[1]\n",
    "\n",
    "# Definimos los algoritmos de clasificación.\n",
    "# Aplicamos el modelo sobre el conjunto de entrenamiento.\n",
    "\n",
    "log_rg = LogisticRegression(maxIter=5, regParam=0.0, labelCol=\"admit\")\n",
    "log_rg_model = log_rg.fit(train2)\n",
    "\n",
    "# Obtenemos algunos parámetros del modelo.\n",
    "\n",
    "print(\"Intercept: {0}\".format(log_rg_model.intercept))\n",
    "print(\"Coefficients: {0}\".format(log_rg_model.coefficients))\n",
    "print(\" \")\n",
    "\n",
    "# Hacemos predicciones sobre el conjunto de test.\n",
    "\n",
    "predict_train2 = log_rg_model.transform(train2)\n",
    "predict_test2 = log_rg_model.transform(test2)\n",
    "predict_test2.show(5)\n",
    "\n",
    "# Evaluamos el resultado mostrando el área bajo la curva ROC\n",
    "\n",
    "ev2 = BinaryClassificationEvaluator(labelCol=\"admit\")\n",
    "print(\"Área bajo curva ROC: {0}\".format(ev2.evaluate(predict_test2, {ev2.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='dtc' />\n",
    "\n",
    "#### Clasificación con árboles de desición (Decision Trees)\n",
    "\n",
    "También se puede utilizar este algoritmo para problemas de clasificación. La clase correspondiente es `DecisionTreeClassifier?`. Soporta variables predictoras de tipo *continuo* o *categóricas* y predicciones *multiclase*.\n",
    "\n",
    "<div id='gbtc' />\n",
    "\n",
    "#### Clasificador GBT\n",
    "\n",
    "La clase python que utilizamos para la implementación de modelos de clasificación con *árboles GBT* es `GBTClassifier?`. Soporta variables predictoras de tipo *continuo* o *categóricas* y realiza predicciones sobre variables binarias (no soporta clasificación *multiclase*).\n",
    "\n",
    "<div id='rfc' />\n",
    "\n",
    "#### Clasificador Random Forest\n",
    "\n",
    "`RandomForestClassifier?` es la clase que aplica este modelo. Soporta variables predictoras de tipo *continuo* o *categóricas* y predicciones *multiclase*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+\n",
      "| ID|sepal_length|sepal_width|petal_length|petal_width|     specie|specienum|         features|\n",
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|      0.0|[5.1,3.5,1.4,0.2]|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|      0.0|[4.9,3.0,1.4,0.2]|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2|Iris-setosa|      0.0|[4.7,3.2,1.3,0.2]|\n",
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+--------------------+--------------------+----------+\n",
      "| ID|sepal_length|sepal_width|petal_length|petal_width|     specie|specienum|         features|       rawPrediction|         probability|prediction|\n",
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+--------------------+--------------------+----------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|      0.0|[5.1,3.5,1.4,0.2]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|      0.0|[4.9,3.0,1.4,0.2]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2|Iris-setosa|      0.0|[4.7,3.2,1.3,0.2]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "| 14|         4.3|        3.0|         1.1|        0.1|Iris-setosa|      0.0|[4.3,3.0,1.1,0.1]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "| 27|         5.0|        3.4|         1.6|        0.4|Iris-setosa|      0.0|[5.0,3.4,1.6,0.4]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy: 0.95\n",
      "Precision: 0.957143\n",
      "Recall: 0.95\n",
      "F1: 0.950137\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.feature import VectorAssembler as VA\n",
    "#from pyspark.sql.typer import *\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Clasificación del dataset iris.csv con RandomForest\n",
    "# Usaremos el dataset iris_csvDF cargado anteriormente.\n",
    "\n",
    "# Primero transformamos la etiqueta specie de string a double en varios niveles.\n",
    "\n",
    "indexer2 = StringIndexer(inputCol=\"specie\", outputCol=\"specienum\")\n",
    "i2_model = indexer2.fit(iris_csvDF)\n",
    "i2_data = i2_model.transform(iris_csvDF)\n",
    "\n",
    "# Construimos un assembler para unir todas las columnas predictoras del dataset en un vector de features.\n",
    "\n",
    "vAssem3 = VA(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
    "vt3 = vAssem3.transform(i2_data)\n",
    "vt3.show(3)\n",
    "\n",
    "# Creamos conjunto entrenamiento y test.\n",
    "\n",
    "splits3 = vt3.randomSplit([0.7, 0.3], 1234)\n",
    "train3 = splits3[0]\n",
    "test3 = splits3[1]\n",
    "\n",
    "# Definimos el algoritmo de RandomForest.\n",
    "# Aplicamos el modelo sobre el conjunto de entrenamiento.\n",
    "\n",
    "rfc = RandomForestClassifier(numTrees=20, maxDepth=2, labelCol=\"specienum\", seed=42)\n",
    "rfc_model = rfc.fit(train3)\n",
    "\n",
    "# Hacemos predicciones sobre el conjunto de test.\n",
    "\n",
    "predict_train3 = rfc_model.transform(train3)\n",
    "predict_test3 = rfc_model.transform(test3)\n",
    "predict_test3.show(5)\n",
    "\n",
    "# Aplicamos el evaluador multiclase para ver la eficiencia del modelo.\n",
    "\n",
    "ev3 = MulticlassClassificationEvaluator(labelCol=\"specienum\")\n",
    "#print(\"Precision: {0}, Recall: {1}, F1: {2}\".format(ev3.evaluate(predict_test3, {ev3.metricName: \"precision\"}), \n",
    "#                                                    ev3.evaluate(predict_test3, {ev3.metricName: \"recall\"}), \n",
    "#                                                    ev3.evaluate(predict_test3, {ev3.metricName: \"f1\"})))\n",
    "\n",
    "print(\"Accuracy: %g\" % (ev3.evaluate(predict_test3, {ev3.metricName: \"accuracy\"})))\n",
    "print(\"Precision: %g\" % (ev3.evaluate(predict_test3, {ev3.metricName: \"weightedPrecision\"})))\n",
    "print(\"Recall: %g\" % (ev3.evaluate(predict_test3, {ev3.metricName: \"weightedRecall\"})))\n",
    "print(\"F1: %g\" % (ev3.evaluate(predict_test3, {ev3.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='nb' />\n",
    "\n",
    "#### Clasificador NaiveBayes\n",
    "\n",
    "Este modelo se implementa en python con la clase `NaiveBayes?` y soporta los tipos *Multinomial NB* y *Bernoulli NB*.\n",
    "\n",
    "El multinomial puede manejar de forma eficiente variables discretas tipo vectores TF-IDF de palabras de documentos para clasificación de textos.\n",
    "\n",
    "El de bernoulli funciona de forma similar al multinomial pero transformando el valor TF-IDF en ceros y unos (0/1) indicando la presencia o no de la palabra correspondiente. Una condición más para este modelo es que los valores de los predictores no pueden ser negativos.\n",
    "\n",
    "<div id='perceptron' />\n",
    "\n",
    "#### Clasificador Perceptrón Multicapa\n",
    "\n",
    "Este modelo viene con la clase `MultilayerPerceptronClassifier?` y presenta las siguientes características:\n",
    "\n",
    "- Cada capa cuenta con una función de activación sigmoide.\n",
    "- La capa de salida tiene **softmax**.\n",
    "- El número de entradas tiene que ser igual al tamaño del vector de predictores *features*.\n",
    "- El número de salidas tiene que ser igual al número total de variables target *labels*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='cv' />\n",
    "\n",
    "### Validación cruzada (Cross Validation)\n",
    "\n",
    "Usaremos el modelo de regresión lineal sobre el dataset *faithful.csv* como ejemplo de validación cruzada. Las diferencias surgen cuando llegamos a la parte de creación del modelo:\n",
    "\n",
    "1) Carga de datos, creación del vector de características (VectorAssembler) y creación de conjunto entrenamiento y test.\n",
    "\n",
    "2) Creación del modelo LinearRegression donde solo pasamos como parámetro la variable target.\n",
    "\n",
    "3) Ajuste con k-fold y grid de parámetros: `regParam` y `elasticNetParam` se pasan al modelo a un objeto de tipo grid en el que se indican los conjuntos de valores que tomarán dichos parámetros iterando en diferentes ejecuciones hasta que se encuentre el valor más óptimo. Es decir, si vemos que el parámetro `regParam` viene acompañado por `[0.0, 0.01, 0.05, 0.5]` significa que habrá una ejecución por cada uno de los valores de la lista. Lo mismo para `elasticNetParam`. Por tanto se realizarán las ejecuciones necesarias para cubrir las distintas combinaciones de ambos parámetros.\n",
    "\n",
    "4) Se crea un objeto tipo `CrossValidator` al que se le pasa como parámetro el modelo, el grid de parámetros, el evaluador (que servirá para ir comparando los modelos y quedarse con el mejor) y el número de iteraciones en el que se irán cambiando los conjuntos de entrenamiento y test mediante cross validation (kfold) que en este caso serán 5 `numFolds = 5`. Así pues, habrá 5 ejecuciones por cada par de valores de `regParam` y `elasticNetParam`, lo que hará subir el número de trabajos ejecutados por Spark.\n",
    "    \n",
    "5) Se aplica la función `fit()` generándose un modelo tipo `CrossValidationModel`. De este tipo de objeto no se pueden obtener el *intercept* y los *coefficients* sino que hay que acceder a su objeto hijo `bestModel` que es de tipo `LinearRegressionModel` y que es el modelo lineal optimizado resultado de todo el proceso iterativo.\n",
    "\n",
    "6) Para realizar predicciones se pueden utilizar el modelo de tipo `CrossValidationModel` o el que corresponde a `bestModel`\n",
    "\n",
    "7) Finalmente hacemos una evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cv_model tipo:  <class 'pyspark.ml.regression.LinearRegressionModel'>\n",
      "Modelo cv_model.bestModel tipo:  <class 'pyspark.ml.regression.LinearRegressionModel'>\n",
      " \n",
      "Intercept: -1.9156822121843768\n",
      "Coefficients: [0.07661202567343066]\n",
      " \n",
      "+---+---------+-------+--------+------------------+\n",
      "|_c0|eruptions|waiting|features|        prediction|\n",
      "+---+---------+-------+--------+------------------+\n",
      "|  1|      3.6|     79|  [79.0]| 4.136667816016645|\n",
      "|  2|      1.8|     54|  [54.0]|2.2213671741808785|\n",
      "|  3|    3.333|     74|  [74.0]|3.7536076876494917|\n",
      "+---+---------+-------+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "RMSE test: 0.459819\n",
      "RMSE training: 0.507946\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.ml.feature import VectorAssembler as VA\n",
    "#from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "#from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Carga de datos.\n",
    "#df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"data/faithful.csv\")\n",
    "\n",
    "# Creamos un VectorAssembler (vector de características) para tener todas las variables predictoras en un vector features.\n",
    "#vAssem = VA(inputCols=[\"waiting\"], outputCol=\"features\")\n",
    "#vt = vAssem.transform(df)\n",
    "#vt.show(3)\n",
    "\n",
    "# Creamos conjunto entrenamiento y test.\n",
    "#splits = vt.randomSplit([0.7, 0.3], 1234)\n",
    "#train = splits[0]\n",
    "#test = splits[1]\n",
    "\n",
    "# Creación del modelo LinearRegression. Solo pasamos como parámetro la variable target.\n",
    "# Ajuste con k-fold y grid de parámetros.\n",
    "\n",
    "lrcv = LinearRegression(labelCol=\"eruptions\")\n",
    "lr_grid = ParamGridBuilder().addGrid(lrcv.regParam, [0.0, 0.01, 0.05, 0.5]).addGrid(lrcv.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "lr_ev = RegressionEvaluator(labelCol=\"eruptions\")\n",
    "cv = CrossValidator(estimator=lrcv, estimatorParamMaps=lr_grid, evaluator=lr_ev, numFolds=5)\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "print(\"Modelo cv_model tipo: \", type(cv_model.bestModel))\n",
    "print(\"Modelo cv_model.bestModel tipo: \", type(cv_model.bestModel))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# Obtenemos algunos parámetros del modelo.\n",
    "\n",
    "print(\"Intercept: {0}\".format(cv_model.bestModel.intercept))\n",
    "print(\"Coefficients: {0}\".format(cv_model.bestModel.coefficients))\n",
    "print(\" \")\n",
    "\n",
    "# Hacemos predicciones sobre el conjunto de test.\n",
    "\n",
    "predict_cv_train = cv_model.transform(train)\n",
    "predict_cv_test = cv_model.transform(test)\n",
    "predict_cv_test.show(3)\n",
    "\n",
    "print(\"RMSE test: %g\" % (lr_ev.evaluate(predict_cv_test, {lr_ev.metricName: \"rmse\"})))\n",
    "print(\"RMSE training: %g\" % (lr_ev.evaluate(predict_cv_train, {lr_ev.metricName: \"rmse\"})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='pipeline' />\n",
    "\n",
    "### Flujos ejecución Spark Pipeline\n",
    "\n",
    "Con Spark ML existe el concepto de *Pipeline* que representa el flujo de trabajo que encadena varios *Transformers* y *Estimators*.\n",
    "\n",
    "A continuación podemos ver como en un mismo **pipeline** se indican como pasos (stages) los transformers y estimators (indexer, vectorAssembler y rf) a ejecutar y el orden en que queremos que se haga. Por último se crea el modelo `pipeModel` aplicando mediante la función `fit()` el pipeline al conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+--------------------+--------------------+----------+\n",
      "| ID|sepal_length|sepal_width|petal_length|petal_width|     specie|specienum|         features|       rawPrediction|         probability|prediction|\n",
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+--------------------+--------------------+----------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|      0.0|[5.1,3.5,1.4,0.2]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|      0.0|[4.9,3.0,1.4,0.2]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2|Iris-setosa|      0.0|[4.7,3.2,1.3,0.2]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "| 14|         4.3|        3.0|         1.1|        0.1|Iris-setosa|      0.0|[4.3,3.0,1.1,0.1]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "| 27|         5.0|        3.4|         1.6|        0.4|Iris-setosa|      0.0|[5.0,3.4,1.6,0.4]|[19.9555555555555...|[0.99777777777777...|       0.0|\n",
      "+---+------------+-----------+------------+-----------+-----------+---------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.ml.feature import VectorAssembler as VA\n",
    "#from pyspark.ml.classification import RandomForestClassifier\n",
    "#from pyspark.ml.feature import StringIndexer\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Clasificación del dataset iris.csv con RandomForest mediante Pipeline.\n",
    "# Usaremos el dataset iris_csvDF cargado anteriormente.\n",
    "\n",
    "# Primero transformamos la etiqueta specie de string a double en varios niveles.\n",
    "indexer_pip = StringIndexer(inputCol=\"specie\", outputCol=\"specienum\")\n",
    "\n",
    "# Construimos un assembler para unir todas las columnas predictoras del dataset en un vector de features.\n",
    "vAssem_pip = VA(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
    "\n",
    "# Creamos conjunto entrenamiento y test.\n",
    "\n",
    "splits_pip = iris_csvDF.randomSplit([0.7, 0.3], 1234)\n",
    "train_pip = splits_pip[0]\n",
    "test_pip = splits_pip[1]\n",
    "\n",
    "\n",
    "# Definimos el algoritmo de RandomForest.\n",
    "rfc_pip = RandomForestClassifier(numTrees=20, maxDepth=2, labelCol=\"specienum\", seed=42)\n",
    "\n",
    "# Definimos el Pipeline con las operaciones en orden.\n",
    "pipeline = Pipeline(stages= [indexer_pip, vAssem_pip, rfc_pip])\n",
    "\n",
    "# Creamos el modelo aplicandoel pipeline al conjunto de entrenamiento.\n",
    "pipeline_model = pipeline.fit(train_pip)\n",
    "\n",
    "# Hacemos predicciones sobre el conjunto de test.\n",
    "\n",
    "predict_train_pip = pipeline_model.transform(train_pip)\n",
    "predict_test_pip = pipeline_model.transform(test_pip)\n",
    "predict_test3.show(5)\n",
    "\n",
    "#print(\"Accuracy: %g\" % (ev3.evaluate(predict_test3, {ev3.metricName: \"accuracy\"})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='mllib' />\n",
    "\n",
    "## Spark.mllib\n",
    "\n",
    "[Spark.mllib](https://spark.apache.org/docs/latest/mllib-guide.html) es una librería alternativa para hacer machine learning sobre RDDs. Vamos a ver un ejemplo con un modelo RandomForest para comparar con los resultados anteriores. En este [enlace](https://spark.apache.org/docs/latest/mllib-ensembles.html#random-forests) tpodemos ver la guía del modelo. Los puntos distintos más relevantes de mllib con respecto a ml son los siguientes:\n",
    "\n",
    "- Se utilizan RDDs en vez de dataframes y *Labeled Points* en vez de *Vectors*.\n",
    "\n",
    "- El modelo se crea de una pasada (no es necesario aplicar el estimador con la función `fit()`.\n",
    "\n",
    "- Para predecir se utiliza la función `predict()` en vez de `transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [1.0,5.1,3.5,1.4,0.2]), LabeledPoint(1.0, [2.0,4.9,3.0,1.4,0.2]), LabeledPoint(1.0, [3.0,4.7,3.2,1.3,0.2])]\n",
      " \n",
      "Número de nodos de los árboles: 5\n",
      " Modelo RandomForest de entrenamiento: \n",
      "TreeEnsembleModel classifier with 1 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 0 <= 102.5)\n",
      "     If (feature 3 <= 2.45)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 2.45)\n",
      "      Predict: 2.0\n",
      "    Else (feature 0 > 102.5)\n",
      "     Predict: 0.0\n",
      "\n",
      "Predicción sobre test: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      " \n",
      "Test Error = 0.02702702702702703\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "# Creamos una función para pasar las etiquetas a numérico.\n",
    "\n",
    "def getLabelNum(label):\n",
    "    if label == \"Iris-virginica\":\n",
    "        return 0.0\n",
    "    elif label == \"Iris-setosa\":\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 2.0\n",
    "\n",
    "# Cargamos datos y eliminamos cabeceras\n",
    "\n",
    "rdd = sc.textFile(\"../data/iris.csv\")\n",
    "\n",
    "headers = rdd.first()\n",
    "rdd = rdd.filter(lambda line: line != headers)\n",
    "\n",
    "rdd = rdd.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Metemos cada uno de los campos separados por coma en un punto etiquetado.\n",
    "\n",
    "rdd = rdd.map(lambda x: LabeledPoint(getLabelNum(x[5]), [x[0], x[1], x[2], x[3], x[4]]))\n",
    "print(rdd.take(3))\n",
    "\n",
    "# Dividimos conjunto de datos en entrenamiento y test\n",
    "\n",
    "(trainingData, testData) = rdd.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Creamos modelo RandomForest.\n",
    "\n",
    "rdd_model = RandomForest.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={}, numTrees=1, maxDepth=2, seed=42)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Número de nodos de los árboles: {0}\".format(rdd_model.totalNumNodes()))\n",
    "print(\" Modelo RandomForest de entrenamiento: \")\n",
    "print(rdd_model.toDebugString())\n",
    "\n",
    "# Creamos un RDD sobre el que hacer predicciones. Hacemos predicciones sobre conjunto test.\n",
    "\n",
    "predictions = rdd_model.predict(testData.map(lambda x: x.features))\n",
    "print(\"Predicción sobre test: {0}\".format(predictions.collect()))\n",
    "\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Test Error = \" + str(testErr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
