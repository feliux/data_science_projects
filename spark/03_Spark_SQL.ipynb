{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "\n",
    "- [Spark SQL](#sql)\n",
    "    - [Dataframes](#df)\n",
    "        - [Creación de Dataframes](#newdf)\n",
    "        - [Operaciones con Dataframes](#operations)\n",
    "            - [Transformaciones](#transforms)\n",
    "            - [Acciones](#actions)\n",
    "\n",
    "    - [SQL Queries](#queries)\n",
    "    - [HIVE Queries](#hive)\n",
    "\n",
    "<div id=\"sql\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sql\" />\n",
    "\n",
    "# Spark SQL\n",
    "\n",
    "[Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) es uno de los módulos que forma parte del stack de Spark y que está destinado al procesado de datos estructurados. Las interfaces relacionadas con Spark SQL proporciona más información sobre los datos y la forma de computarlos que los RDD, dicha información se utiliza para realizar optimización que mejora el rendimiento.\n",
    "\n",
    "Hay diversas formas de interactuar con Spark SQL, el API de los **Dataframes** y el de los **Datasets**. Todas ellas comparten la misma máquina de ejecución y es muy sencillo combinar distintos medios de interacción con Spark SQL de tal manera que los desarrolladores elijan en cada momento el que sea más cómodo.\n",
    "\n",
    "Spark SQL proporciona objetos para el acceso y tratamiento de datos denominados **Dataframes** y un motor de consultas distribuido. Es capaz de localizar tablas y metadatos ya existentes en HIVE sin necesidad de realizar ningún trabajo extra (auqnue requiere cierto trabajo de configuración). También se puede interactuar con Spark SQL a través de línea de comandos mediante la herramienta *spak-sql* o utilizando JDBC/ODBC.\n",
    "\n",
    "<div id=\"df\" />\n",
    "\n",
    "## Dataframes de Spark\n",
    "\n",
    "Los dataframes de Spark equivalen a una tabla en una base de datos relacional o a un dataframe de python o R pero que almacena sus datos de manera distribuida y que cuenta con optimizaciones avanzadas orientadas a facilitar el análisis de datos a gran escala.\n",
    "\n",
    "El dataframe de Spark SQL es equivalente al RDD en Spark Core pero con la diferencia fundamental de que los datos son estructurados. Para crear un dataframe necesitamos una clase *SparkContext*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uno u otro\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, HiveContext\n",
    "from pyspark import SparkContext\n",
    "#from pyspark import SparkConf\n",
    "\n",
    "#SPARK_CONF = SparkConf().setMaster(\"yarn-clietn\").setAppName(\"app_name\")\n",
    "#spark = SparkSession.builder.config(conf=SPARK_CONF).enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "sqlc = SQLContext(sc)\n",
    "hivec = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Spark dataframes tienen una serie de características:\n",
    "\n",
    "- Son inmutables: no se pueden cambiar una vez construidos\n",
    "- Mantienen información de los planes de ejecución que permite recomputar lps datos perdidos de manera eficiente en caso de fallo\n",
    "- Permiten realizar operaciones en colecciones de elementos en paralelo.\n",
    "\n",
    "El ciclo de programación con los dataframes es muy similar al de los RDD:\n",
    "\n",
    "1) Crear Dtaframe: desde dtos externos, colecciones o RDDs existentes.\n",
    "\n",
    "2) Transformar los dataframes en nuevos dataframes.\n",
    "\n",
    "3) *Cachear* algún dataframe para reusarlos posteriormente\n",
    "\n",
    "4) Ejecutar acciones que corran en paralelo y produzcan resultados.\n",
    "\n",
    "Spark SQL soporta los siguientes tipos de datos:\n",
    "\n",
    "\n",
    "![alt text](../images/data_type.png \"Tipos de datos\")\n",
    "\n",
    "\n",
    "Para importar todos los tipos de datos y poderlos usar en nuestros programas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"newdf\" />\n",
    "\n",
    "### Creación de dataframes\n",
    "\n",
    "- **A partir de una lista python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Data [(4.3, 3.0, 1.1, 0.1, 'Iris-setosa'), (7.0, 3.2, 4.7, 1.4, 'Iris-versicolor'), (6.9, 3.2, 5.7, 2.3, 'Iris-virginica')]\n",
      " \n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|         specie|\n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "|         4.3|        3.0|         1.1|        0.1|    Iris-setosa|\n",
      "|         7.0|        3.2|         4.7|        1.4|Iris-versicolor|\n",
      "|         6.9|        3.2|         5.7|        2.3| Iris-virginica|\n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=4.3, sepal_width=3.0, petal_length=1.1, petal_width=0.1, specie='Iris-setosa'),\n",
       " Row(sepal_length=7.0, sepal_width=3.2, petal_length=4.7, petal_width=1.4, specie='Iris-versicolor'),\n",
       " Row(sepal_length=6.9, sepal_width=3.2, petal_length=5.7, petal_width=2.3, specie='Iris-virginica')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data = [(4.3, 3.0, 1.1, 0.1, \"Iris-setosa\"), (7.0, 3.2, 4.7, 1.4, \"Iris-versicolor\"), (6.9, 3.2, 5.7, 2.3, \"Iris-virginica\")]\n",
    "print(\"Iris Data\", iris_data)\n",
    "print(\" \")\n",
    "df = spark.createDataFrame(iris_data, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"specie\"])\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A partir de un dataframe de pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---------------+\n",
      "|  0|  1|  2|  3|              4|\n",
      "+---+---+---+---+---------------+\n",
      "|4.3|3.0|1.1|0.1|    Iris-setosa|\n",
      "|7.0|3.2|4.7|1.4|Iris-versicolor|\n",
      "|6.9|3.2|5.7|2.3| Iris-virginica|\n",
      "+---+---+---+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame(iris_data)\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A partir de un RDD.** Spark SQL soporta dos métodos diferentes para convertir RDD ya existentes en dataframes. El primer método usa **reflection** para inferir el esquema de un RDD que contiene tipos específicos de objetos. Esta aproximación utiliza un código muy conciso y funciona bien cuando el esquema ya es conocido al programar la aplicación Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "| ID|petal_length|petal_width|sepal_length|sepal_width|     specie|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "|  1|         1.4|        0.2|         5.1|        3.5|Iris-setosa|\n",
      "|  2|         1.4|        0.2|         4.9|        3.0|Iris-setosa|\n",
      "|  3|         1.3|        0.2|         4.7|        3.2|Iris-setosa|\n",
      "|  4|         1.5|        0.2|         4.6|        3.1|Iris-setosa|\n",
      "|  5|         1.4|        0.2|         5.0|        3.6|Iris-setosa|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "iris = sc.textFile(\"../data/iris.csv\")\n",
    "\n",
    "# Eliminamos cabeceras\n",
    "headers = iris.first()\n",
    "rdd = iris.filter(lambda line: line != headers)\n",
    "\n",
    "rdd = rdd.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Metemos cada uno de los campos separados por coma en un objeto tipo fila (Row) y le damos un nombre\n",
    "rdd = rdd.map(lambda x: Row(ID=int(x[0]), sepal_length=float(x[1]), sepal_width=float(x[2]), petal_length=float(x[3]), petal_width=float(x[4]), specie=x[5]))\n",
    "#print(rdd.take(5))\n",
    "\n",
    "irisDF = spark.createDataFrame(rdd)\n",
    "irisDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda forma de crear un dataframe a partir de un RDD se conoce como **programática** y es especialmente útil cuando el diccionario de campos no se conoce antes de la creación del propio dataframe: por ejemplo cuando la estructura de campos está codificada en una cadena de caracteres y hay que leerlos desde ahí.\n",
    "\n",
    "Los pasos a seguir en este tipo de creaciones es el siguiente:\n",
    "\n",
    "1) Crear un RDD de tuplas o listas desde el RDD original (habitualmente usando la función \"map\"). \n",
    "\n",
    "2) Crear un esquema representado por un tipo StructType que coincida con la estructura de las tuplas o listas del RDD del punto anterior.\n",
    "\n",
    "3) Aplicar el esquema RDD vía método createDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "| ID|sepal_length|sepal_width|petal_length|petal_width|     specie|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|  4|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|  5|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.types import *\n",
    "\n",
    "rdd = iris.filter(lambda line: line != headers)\n",
    "\n",
    "rdd = rdd.map(lambda x: x.split(\",\"))\n",
    "rdd = rdd.map(lambda x: (int(x[0]), float(x[1]), float(x[2]), float(x[3]), float(x[4]), x[5]))\n",
    "\n",
    "# Definimos el nombre y tipo de campos para nuestro dataframe\n",
    "fields = [StructField(\"ID\", IntegerType(), True), StructField(\"sepal_length\", FloatType(), True), StructField(\"sepal_width\", FloatType(), True), StructField(\"petal_length\", FloatType(), True), StructField(\"petal_width\", FloatType(), True), StructField(\"specie\", StringType(), True)]\n",
    "\n",
    "# Definimos el esquema con los tipos de campos creados\n",
    "schema = StructType(fields)\n",
    "\n",
    "irisDF2 = spark.createDataFrame(rdd, schema)\n",
    "irisDF2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Es posible crear un RDD a partir de un dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.099999904632568, sepal_width=3.5, specie='Iris-setosa'),\n",
       " Row(sepal_length=4.900000095367432, sepal_width=3.0, specie='Iris-setosa'),\n",
       " Row(sepal_length=4.699999809265137, sepal_width=3.200000047683716, specie='Iris-setosa'),\n",
       " Row(sepal_length=4.599999904632568, sepal_width=3.0999999046325684, specie='Iris-setosa'),\n",
       " Row(sepal_length=5.0, sepal_width=3.5999999046325684, specie='Iris-setosa')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisRDD = irisDF2.select(\"sepal_length\", \"sepal_width\", \"specie\").rdd\n",
    "irisRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A partir de un fichero JSON.** El fichero tiene que cumplir ciertos requisitos: cada línea debe contener un objeto separado, válido y autocontenido, es decir, un fichero JSON convencional del tipo multilínea fallará casi con el 100% de seguridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|_corrupt_record|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|              [|       null|      null|       null|      null|   null|\n",
      "|           null|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|           null|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "|           null|        1.3|       0.2|        4.7|       3.2| setosa|\n",
      "|           null|        1.5|       0.2|        4.6|       3.1| setosa|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.types import *\n",
    "iris_jsonDF = spark.read.format(\"json\").load(\"../data/iris.json\")\n",
    "iris_jsonDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A partir de un fichero csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "| ID|sepal_length|sepal_width|petal_length|petal_width|     specie|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.types import *\n",
    "\n",
    "#iris_csvDF = spark.read.format(\"csv\").options(header=True).load(\"../data/iris.csv\")\n",
    "#iris_csvDF = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"../data/iris.csv\")\n",
    "iris_csvDF = spark.read.format(\"csv\").options(header=True).load(\"../data/iris.csv\", schema=schema)\n",
    "iris_csvDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A partir de una tabla HIVE.** Siempre que tengamos activado el soporte HIVE activado (*enableHiveSupport()*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debe existir en Hive previamente la tabla \"iris-tabla-hive\" en la base de datos \"test\"\n",
    "\n",
    "#irisHIVE = spark.table(\"test.iris-tabla-hive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"operations\" />\n",
    "\n",
    "### Operaciones con dataframes\n",
    "\n",
    "Al igual que con los RDD, con los dataframes podemos realizar dos tipos de operaciones: **transformaciones** y **acciones**. También al igual que con los RDD, las transformaciones son *lazy*, es decir, que no se computan inmediatamente sino que son modifiados de forma efectiva cuando se aplica una **acción** sobre ellos.\n",
    "\n",
    "<div id=\"transforms\" />\n",
    "\n",
    "#### Transformaciones\n",
    "\n",
    "El resultado de una **transformación** es otro dataframe.\n",
    "\n",
    "Algunos ejemplos de transformaciones sobre un dataframe son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|     specie|\n",
      "+-----------+\n",
      "|Iris-setosa|\n",
      "|Iris-setosa|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisSpecie = irisDF.select(\"specie\")\n",
    "irisSpecie.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|petal_length|     specie|\n",
      "+------------+-----------+\n",
      "|         1.4|Iris-setosa|\n",
      "|         1.4|Iris-setosa|\n",
      "+------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisSpecie2 = irisDF.select(irisDF.petal_length, irisDF.specie)\n",
    "irisSpecie2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|petal_length + 1|     specie|\n",
      "+----------------+-----------+\n",
      "|             2.4|Iris-setosa|\n",
      "|             2.4|Iris-setosa|\n",
      "+----------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisSpecie3 = irisDF.select((irisDF.petal_length + 1).alias(\"petal_length + 1\"), irisDF.specie)\n",
    "irisSpecie3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+\n",
      "| ID|petal_length|petal_width|sepal_length|sepal_width|\n",
      "+---+------------+-----------+------------+-----------+\n",
      "|  1|         1.4|        0.2|         5.1|        3.5|\n",
      "|  2|         1.4|        0.2|         4.9|        3.0|\n",
      "+---+------------+-----------+------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisSinSpecie = irisDF.drop(irisDF.specie)\n",
    "irisSinSpecie.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***filter(func)*** devuelve un dataframe resultado de la selección de las filas del dataframe original en las que la aplicación de la función retorna \"True\" (se cumple la condición)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas antes del filtrado:  150\n",
      "Número de filas después del filtrado:  118\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de filas antes del filtrado: \", irisDF.count())\n",
    "\n",
    "irisFiltrado = irisDF.filter(irisDF.sepal_length > 5.0)\n",
    "\n",
    "print(\"Número de filas después del filtrado: \", irisFiltrado.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***distinct()*** devuelve un dataframe que contiene solamente las filas distintas del dataframe original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas totales:  150\n",
      "Número de filas NO repetidas:  150\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de filas totales: \", irisDF.count())\n",
    "print(\"Número de filas NO repetidas: \", irisDF.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***orderby(cols, kw)*** crea un nuevo dataframe ordenado por las columnas especificadas (cols) y en el sentido indicado (kw).\n",
    "\n",
    "***sort(cols, kw)*** tiene el mismo comportamiento que orderby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+--------------+\n",
      "| ID|petal_length|petal_width|sepal_length|sepal_width|        specie|\n",
      "+---+------------+-----------+------------+-----------+--------------+\n",
      "|103|         5.9|        2.1|         7.1|        3.0|Iris-virginica|\n",
      "|101|         6.0|        2.5|         6.3|        3.3|Iris-virginica|\n",
      "+---+------------+-----------+------------+-----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisDF.sort(\"specie\", ascending = False).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***explode(col)*** retorna un nuevo dataframe en el que se crea una nueva fila por cada uno de los elementos de una columna que contenga un dato tipo array o map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|   sepal_length_list|   specie|\n",
      "+--------------------+---------+\n",
      "|[0.1, 2.3, 3.1, 4.0]|Virginica|\n",
      "+--------------------+---------+\n",
      "\n",
      "+---------+------------+\n",
      "|   specie|sepal_length|\n",
      "+---------+------------+\n",
      "|Virginica|         0.1|\n",
      "|Virginica|         2.3|\n",
      "|Virginica|         3.1|\n",
      "|Virginica|         4.0|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "data = [Row(specie=\"Virginica\", sepal_length_list=[0.1, 2.3, 3.1, 4.0])]\n",
    "\n",
    "irisE = spark.createDataFrame(data)\n",
    "irisE.show()\n",
    "\n",
    "irisExplode = irisE.select(irisE.specie, explode(irisE.sepal_length_list).alias(\"sepal_length\"))\n",
    "irisExplode.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***udf()*** user-defined-functions. Además de las funciones que podemos crear nosotros mismos, Spark cuenta con una lista muy extensa de funciones predefinidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|     specie|sepal_length + 1|\n",
      "+-----------+----------------+\n",
      "|Iris-setosa|             6.1|\n",
      "|Iris-setosa|             5.9|\n",
      "|Iris-setosa|             5.7|\n",
      "+-----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "masUno = udf(lambda x: x + 1, FloatType())\n",
    "\n",
    "irisDFMasUno = irisDF.select(irisDF.specie, masUno(irisDF.sepal_length).alias(\"sepal_length + 1\"))\n",
    "irisDFMasUno.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos hacer transformaciones de agrupación sobre dataframes:\n",
    "\n",
    "***groupby()*** agrupa el dataframe utilizando las columnas especificadas, pudiendo después aplicar operaciones de agregación sobre ellos.\n",
    "\n",
    "***agg()*** calcula agregados (avg, max, min, sum y count) y retorna los resultados como un dataframe.\n",
    "\n",
    "***count()*** cuenta el número de filas.\n",
    "\n",
    "***avg() mean()*** calcula los valores medios de las columnas por cada grupo.\n",
    "\n",
    "***max() min() sum()***\n",
    "\n",
    "***pivot(col, values)*** pivota una columna del dataframe y calcula el agregado correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|         specie|count(1)|\n",
      "+---------------+--------+\n",
      "| Iris-virginica|      50|\n",
      "|    Iris-setosa|      50|\n",
      "|Iris-versicolor|      50|\n",
      "+---------------+--------+\n",
      "\n",
      "+---------------+-----------------+------------------+\n",
      "|         specie|avg(sepal_length)|  avg(sepal_width)|\n",
      "+---------------+-----------------+------------------+\n",
      "| Iris-virginica|6.587999999999998|2.9739999999999998|\n",
      "|    Iris-setosa|5.005999999999999|3.4180000000000006|\n",
      "|Iris-versicolor|            5.936|              2.77|\n",
      "+---------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "irisG = irisDF.groupby(irisDF.specie)\n",
    "\n",
    "irisGCount = irisG.agg({\"*\": \"count\"})\n",
    "irisGCount.show()\n",
    "\n",
    "irisGAVG = irisG.agg(avg(\"sepal_length\"), avg(\"sepal_width\"))\n",
    "irisGAVG.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         specie|count|\n",
      "+---------------+-----+\n",
      "| Iris-virginica|   50|\n",
      "|    Iris-setosa|   50|\n",
      "|Iris-versicolor|   50|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-------+-----------------+------------------+-----------------+------------------+\n",
      "|         specie|avg(ID)|avg(petal_length)|  avg(petal_width)|avg(sepal_length)|  avg(sepal_width)|\n",
      "+---------------+-------+-----------------+------------------+-----------------+------------------+\n",
      "| Iris-virginica|  125.5|            5.552|             2.026|6.587999999999998|2.9739999999999998|\n",
      "|    Iris-setosa|   25.5|            1.464|0.2439999999999999|5.005999999999999|3.4180000000000006|\n",
      "|Iris-versicolor|   75.5|             4.26|1.3260000000000003|            5.936|              2.77|\n",
      "+---------------+-------+-----------------+------------------+-----------------+------------------+\n",
      "\n",
      "+---------------+-----------------+\n",
      "|         specie|avg(sepal_length)|\n",
      "+---------------+-----------------+\n",
      "| Iris-virginica|6.587999999999998|\n",
      "|    Iris-setosa|5.005999999999999|\n",
      "|Iris-versicolor|            5.936|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Otra manera directamente sobre el dataframe\n",
    "\n",
    "irisG.count().show()\n",
    "irisG.avg().show()\n",
    "irisG.avg(\"sepal_length\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tipos de join: ***inner, outer, left_outer, right_outer, leftsemi***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+--------------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|        specie|  color|\n",
      "+------------+-----------+------------+-----------+--------------+-------+\n",
      "|         6.9|        3.2|         5.7|        2.3|Iris-virginica|Violeta|\n",
      "|         4.3|        3.0|         1.1|        0.1|   Iris-setosa|Naranja|\n",
      "+------------+-----------+------------+-----------+--------------+-------+\n",
      "\n",
      "+------------+-----------+------------+-----------+---------------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|         specie|  color|\n",
      "+------------+-----------+------------+-----------+---------------+-------+\n",
      "|         6.9|        3.2|         5.7|        2.3| Iris-virginica|Violeta|\n",
      "|         4.3|        3.0|         1.1|        0.1|    Iris-setosa|Naranja|\n",
      "|         7.0|        3.2|         4.7|        1.4|Iris-versicolor|   null|\n",
      "+------------+-----------+------------+-----------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#iris_data = [(4.3, 3.0, 1.1, 0.1, \"Iris-setosa\"), (7.0, 3.2, 4.7, 1.4, \"Iris-versicolor\"), (6.9, 3.2, 5.7, 2.3, \"Iris-virginica\")]\n",
    "iris_col = [(\"Iris-setosa\", \"Naranja\"), (\"Iris-virginica\", \"Violeta\")]\n",
    "\n",
    "#df = spark.createDataFrame(iris_data, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"specie\"])\n",
    "iris_df = df\n",
    "iris_col_df = spark.createDataFrame(iris_col, [\"especie\", \"color\"])\n",
    "\n",
    "# Inner join\n",
    "iris_df.join(iris_col_df, iris_df.specie == iris_col_df.especie, \"inner\").select(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"specie\", \"color\").show()\n",
    "\n",
    "# Left outer join\n",
    "iris_df.join(iris_col_df, iris_df.specie == iris_col_df.especie, \"left_outer\").select(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"specie\", \"color\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark cuenta también con transformaciones de tipo estadístico y tratamiento de valores nulos o desconocidos:\n",
    "\n",
    "***corr()*** obtiene el índice de correlación entre dos columnas del datraframe de tipo numérico. Aplicada sobre un dataframe devuelve un tipo numérico mientras que aplicada sobre las columnas con select() o add() devuelve otra columna.\n",
    "\n",
    "***cov()*** similar a corr() pero calcula la covarianza.\n",
    "\n",
    "***crosstab()*** calcula el número de pares de los elementos de dos columnas.\n",
    "\n",
    "***freqItems()*** busca elementos frecuentes en dos columnas a partir de un número mínimo de apariciones.\n",
    "\n",
    "***sampleBy()*** realiza una selección aleatoria de observaciones en base a una proporción prefijada de valores de una variable del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlación sepal_petal_length: 0.8717541656669878\n",
      "+-----------------------+----------------------+\n",
      "|corr_sepal_petal_length|corr_sepal_petal_width|\n",
      "+-----------------------+----------------------+\n",
      "|     0.8717541656669878|  -0.35654410088264576|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, corr, sum\n",
    "\n",
    "print(\"Correlación sepal_petal_length: {0}\".format(iris_csvDF.corr(\"sepal_length\", \"petal_length\")))\n",
    "\n",
    "iris_csvDF.select(corr(\"sepal_length\", \"petal_length\").alias(\"corr_sepal_petal_length\"), corr(\"sepal_width\", \"petal_width\").alias(\"corr_sepal_petal_width\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***replace()*** actúa sobre observaciones con valores desconocidos.\n",
    "\n",
    "***fill()** reemplaza valores nulos por otro valor.\n",
    "\n",
    "***drop()*** borra las filas del dataframe que tienen valor null en las columnas que se decida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+---------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|         specie|\n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "|         4.3|        3.0|         0.0|        0.1|    Iris-setosa|\n",
      "|         7.0|        3.2|         4.7|        1.4|Iris-versicolor|\n",
      "|         6.9|        3.2|         5.7|        0.0| Iris-virginica|\n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "\n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|         specie|\n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "|         4.3|        3.0|         1.0|        0.1|    Iris-setosa|\n",
      "|         7.0|        3.2|         4.7|        1.4|Iris-versicolor|\n",
      "|         6.9|        3.2|         5.7|        0.0| Iris-virginica|\n",
      "+------------+-----------+------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_data = [(4.3, 3.0, 0.0, 0.1, \"Iris-setosa\"), (7.0, 3.2, 4.7, 1.4, \"Iris-versicolor\"), (6.9, 3.2, 5.7, 0.0, \"Iris-virginica\")]\n",
    "iris_zero = spark.createDataFrame(iris_data, [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"specie\"])\n",
    "iris_zero.show()\n",
    "iris_zero.na.replace(0.0, 1.0, [\"petal_length\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"actions\" />\n",
    "\n",
    "#### Acciones\n",
    "\n",
    "Hacen que Spark ejecute las transformaciones que ya se hayan programado anteriormente sobre los dataframes. Dicho de otro modo, son mecanismos para obtener resultados y mostrarlos o sacarlos fuera de spark.\n",
    "\n",
    "***show() take() collect() printSchema() count() describe()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|     sepal_length|        sepal_width|      petal_length|       petal_width|        specie|\n",
      "+-------+-----------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|                3|                  3|                 3|                 3|             3|\n",
      "|   mean|6.066666666666667| 3.1333333333333333|3.8333333333333335|1.2666666666666666|          null|\n",
      "| stddev|1.530795000427338|0.11547005383792526| 2.419366308216541|1.1060440015358037|          null|\n",
      "|    min|              4.3|                3.0|               1.1|               0.1|   Iris-setosa|\n",
      "|    max|              7.0|                3.2|               5.7|               2.3|Iris-virginica|\n",
      "+-------+-----------------+-------------------+------------------+------------------+--------------+\n",
      "\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- specie: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"queries\" />\n",
    "\n",
    "## Spark SQL Queries\n",
    "\n",
    "El SQL que soportan los Spark dataframes permite realizar operaciones sobre tablas **temporales** creadas sobre los propios dataframes. Podemos realizar queries, añadir, modificar o borrar registros y crear, modificar o eliminar relaciones.\n",
    "\n",
    "Para utilizar Spark SQL es necesario asociar una tabla temporal al dataframe mediante la aplicación de la función *registerTempTable()*. Es importante saber que los resultados de los *select()* son nuevos dataframes. Por último, el borrado de una tabla temporal no conlleva el borrado del dataframe a la que fue asignada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "|        |iris_table|       true|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisDF.registerTempTable(\"iris_table\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "| ID|petal_length|petal_width|sepal_length|sepal_width|     specie|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "|  1|         1.4|        0.2|         5.1|        3.5|Iris-setosa|\n",
      "|  6|         1.7|        0.4|         5.4|        3.9|Iris-setosa|\n",
      "| 11|         1.5|        0.2|         5.4|        3.7|Iris-setosa|\n",
      "+---+------------+-----------+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from iris_table where sepal_length > 5 limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|         specie|num_specie|\n",
      "+---------------+----------+\n",
      "|    Iris-setosa|        50|\n",
      "|Iris-versicolor|        50|\n",
      "| Iris-virginica|        50|\n",
      "+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select specie, count(*) as num_specie from iris_table group by specie order by specie asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table iris_table\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"hive\" />\n",
    "\n",
    "## Queries sobre HIVE\n",
    "\n",
    "Spark SQL también soporta consultas y escritura sobre Hive. Para ello, Spark debe estar configurado para que lea del almacén de Hive correcto y tendremos que usar la sesión de Spark SQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
