{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "- [Ecosistema Spark](#spark)\n",
    "- [Arquitectura](#arq)\n",
    "\n",
    "- [RDD](#rdd)\n",
    "    - [Operaciones sobre RDD](#op_rdd)\n",
    "        - [Acciones](#actions)\n",
    "        - [Transformaciones](#transforms)\n",
    "        \n",
    "- [Key-Value RDD](#key_rdd)\n",
    "\n",
    "- [Otros elemenos de Spark Core](#otros)\n",
    "    - [Variables broadcast](#vb)\n",
    "    - [Acumuladores](#acum)\n",
    "    \n",
    "- [Ejemplo final](#ejemplo)\n",
    "\n",
    "    \n",
    "<div id='xx' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='spark' />\n",
    "\n",
    "## Ecosistema Spark\n",
    "\n",
    "\n",
    "Documentación [Spark](https://spark.apache.org/docs/latest/index.html)\n",
    "\n",
    "\n",
    "### Spark Core\n",
    "\n",
    "Contiene la funcionalidad básica de Spark, incluyendo componentes para la planificación de tareas, gestión de memoria, recuperación ante fallos, interacción con los sistemas de almacenamiento, etc.\n",
    "\n",
    "Es también el API que define RDDs (Resilient Distribuited Datasets), que son la principal abstracción en Spark y representan las colecciones de objetos distribuidos a través de muchos nodos de cómputo paralela.\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "Es el paquete de Spark para trabajar con datos estructurados. Permite la consulta de datos a través de SQL y es compatible con muchas fuentes de datos, incluyendo tablas Hive, Parquet y JSON.\n",
    "\n",
    "Nos permite entremezclar consultas SQL con las manipulaciones de datos de forma pragmática sobre RDDs en Python, Java y Scala, combinando así SQL con analíticas complejas.\n",
    "\n",
    "### Spark Streaming\n",
    "\n",
    "Es un componente que permite el procesamiento de secuencias de datos en timepo real. Algunos ejemplos pueden ser:\n",
    "\n",
    "- Ficheros de logs generados por servidores web de producción.\n",
    "- Colas de mensajes que contienen las actualizaciones de estado enviados por los usuarios de un servicio web.\n",
    "\n",
    "Este componente proporciona un API para manipular las secuencias de datos que casi coincide con el API RDD del núcleo de Spark, facilitándo el movernos entre aplicaciones que manipulan los datos almacenados en la memoria, en el disco o que llegan en tiempo real. Además, fue diseñado para proporcionar el mismo grado de tolerancia a fallos, rendimiento y escabilidad que ofrece Spark Core.\n",
    "\n",
    "### MLib\n",
    "\n",
    "Es la librería de spark que contiene machine learning. Todos los métodos de esta librería están diseñados para escalar a través de un clúster.\n",
    "\n",
    "### GraphX\n",
    "\n",
    "Es una librería que nos proporciona un API para la manipulación de grafos y realizar cálculos de grafos en paralelo con gran rendimiento. Extiende el Spark RDD API permitiéndonos crear grafos con propiedades arbitrarias anexas a cada vértice y enlace.\n",
    "\n",
    "### Clúster managers\n",
    "\n",
    "Spark está diseñado para escalar desde uno a cientos de nodos de computación, por ello, para maximizar la flexibilidad Spark puede correr sobre diferentes gestores de clústeres tales como YARN, Apache Mesos o en modo Standalone.\n",
    "\n",
    "<div id='arq' />\n",
    "\n",
    "## Arquitectura\n",
    "\n",
    "Cada aplicación Spark consuste en un programa driver que lanza varias operaciones de computación en paralelo sobre varios nodos 'workers' (también llamados 'executo'). El driver contiene las principales funciones de nuestra aplicación y define y distribuye los datasets en el clúster aplicando operaciones sobre ellos.\n",
    "\n",
    "Para que el programa acceda a Spark, necesitamos crear un objeto <b>SparkContext</b>. En la shell de spark/pyspark se crea automáticamente una variable con nombre **sc** que es el SparkContext.\n",
    "\n",
    "Este SparkContext es la parte principal del API de Spark y representa el punto de entrada a todas las funcionalidades de Spark. Al iniciarlo estaremos definiento el tipo de instalación de Spark a la que queremos conectarnos (local, standalone, mesos, yarn) para ejecutar nuestras operaciones.\n",
    "\n",
    "El esquema de ejecución de una aplicación de Spark sobre YARN es el siguiente:\n",
    "\n",
    "1. Se invoca al ResourceManager al ejecutar una aplicación Spark.\n",
    "2. El ResourceManager es el encargado de consultar el estado de los nodos, solicitando recursos para la aplicación y levantando un ApplicationMaster en uno de los nodos worker, el cual será el encargado de monitorizar los spark executors e irá informando del estado de la aplicación al cliente spark.\n",
    "\n",
    "<div id='rdd' />\n",
    "\n",
    "## RDD\n",
    "\n",
    "Documentación [Spark RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "Como ya hemos comentado con anterioridad, se trata de la principal abstracción y unidad fundamental de datos Spark:\n",
    "\n",
    "- **R**esistente: si los datos en memoria de spierden pueden ser recuperados.\n",
    "- **D**istribuido: los datos son almacenados de forma distribuida a lo largo del clúster y se accede a ellos mediante programación paralela.\n",
    "\n",
    "Un RDD es una colección inmutable de objetos distribuidos. spark es el encargado de distribuir de forma automática los datos contenidos en un RDD en el clúster y paraleliza las operaciones que realicemos sobre dicho RDD.\n",
    "\n",
    "Existen dos formas de crear un RDD. La manera más sencilla es coger una lista que tengamos en memoria y paralelizarla mediante el método *paralelize()* (no muy utilizado ya que se necesita cargar en memoria la colección de datos entera). Lo más habitual en entornos de producción es leer de un fichero externo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comentado ya que solo podemos trabajar sobre una instancia SparkContext\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "#rdd = sc.parallelize(range(100))\n",
    "#rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "rdd = sc.textFile(\"../data/test.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='op_rdd' />\n",
    "\n",
    "### Operaciones sobre RDDs\n",
    "\n",
    "Existen dos tipos de operaciones sobre un RDD:\n",
    "\n",
    "<div id='actions' />\n",
    "\n",
    "#### Acciones\n",
    "\n",
    "Operaciones que se ejecutan sobre un RDD y nos retornan un resultado. escriben información en disco y ejecutan la computación. Por ejemplo: *count()* o *first()*.\n",
    "\n",
    "La ejecución de las acciones se lleva a cabo de forma distribuida en los nodos executors pero el resultado final siempre es un objeto local en el nodo driver, por lo tanto es impoortante tener en cuenta el tipo de acción a ejecutar (el resultado debe ser lo suficientemente pequeño para no colapsar la memoria).\n",
    "\n",
    "Cada vez que invocamos una nueva acción, el RDD se recalcula de nuevo. Para evitar esta ineficiencia si repetimos varias acciones sobre un mismo RDD podemos persistir los resultados intermedios mediante los métodos *persist()* o *cache()*.\n",
    "\n",
    "Cuando persistimos un RDD, cada nodo persiste en su memoria la parte del RDD que almacenaba en disco en ese nodo y acelerando así la reutilización para las distintas acciones que se realicen sobre él. Spark de forma automática se encarga de monitorizar el uso de la cache de cada nodo eliminando las particiones más antiguas que no se han utilizado recientemente. Si queremos eliminar manualmente el RDD de la caché se puede utilizar el método *unpersist()*.\n",
    "\n",
    "A continuación las acciones más habituales sobre un RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()\n",
    "#rdd.first()\n",
    "\n",
    "# Retorna 5 elementos del rdd\n",
    "#rdd.take(5)\n",
    "\n",
    "# Retorna el contenido del rdd.\n",
    "#print(rdd.collect())\n",
    "\n",
    "# Guarda el rdd en disco en el formato XXX (TextFile, SequenceFile, ObjectFile, PickleFile)\n",
    "#rdd.saveAsXXX()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='transforms' />\n",
    "\n",
    "#### Transformaciones\n",
    "\n",
    "Operaciones que se ejecutan sobre un RDD y nos devuelvren un nuevo RDD. Por ejemplo *map()* o *filter()*. \n",
    "\n",
    "Una de las principales características de Spark es que es **lazy evaluation** (evaluación perezosa), esto significa que todas las transformaciones sobre un RDD no se ejecutan hasta que se ejecute una acción.\n",
    "\n",
    "Las transformaciones siempre tienen lugar en los nodos ejecutores.\n",
    "\n",
    "Cuando invocamos una transformación sobre un RDD, spark almacena el tipo de transformación en el siguiente RDD que queremos crear, construyendo internamente un grafo de transformaciones. Este grafo se llama DAG (grafo dirigido acíclico). Spark usa esta información para generar cada RDD cuando se demanda y también para recuperar datos perdidos.\n",
    "\n",
    "Las transformaciones más habituales son:\n",
    "\n",
    "- **rdd.filter()** eliminar elementos del rdd.\n",
    "- **rdd.map()** modificar elementos del rdd.\n",
    "- **rdd.flatMap()** modificar elementos del rdd.\n",
    "- **rdd.join()** unir dos rdd.\n",
    "\n",
    "La mayor parte de las transformaciones e incluso alguna acción dependen de funciones que son usadas por Spark para computar los datos. Si son sencillas usamos expresiones lambda y para casos más complejos se definen previamente funciones python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input had 4 error line\n",
      "64.242.88.10 - - [07/Mar/2004:16:25:35 -0800] \"ERROR /test/error1/primer_error HTTP/1.1\" 404 342\n",
      "64.242.88.10 - - [07/Mar/2004:16:32:51 -0800] \"ERROR /test/error1/segundo_error HTTP/1.1\" 404 4325\n",
      "64.242.88.10 - - [07/Mar/2004:16:58:59 -0800] \"ERROR /test/error3/tercer_error HTTP/1.1\" 404 234125\n",
      "64.242.88.10 - - [07/Mar/2004:17:21:59 -0800] \"ERROR /test/error4/cuarto_error HTTP/1.1\" 404 125\n"
     ]
    }
   ],
   "source": [
    "# Filtrar errores\n",
    "\n",
    "error_rdd = rdd.filter(lambda line: \"ERROR\" in line)\n",
    "print(\"Input had \" + str(error_rdd.count()) + \" error line\")\n",
    "for line in error_rdd.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.242.88.10 - - [07/Mar/2004:16:25:35 -0800] \"ERROR /test/error1/primer_error HTTP/1.1\" 404 342\n",
      "64.242.88.10 - - [07/Mar/2004:16:32:51 -0800] \"ERROR /test/error1/segundo_error HTTP/1.1\" 404 4325\n",
      "64.242.88.10 - - [07/Mar/2004:16:58:59 -0800] \"ERROR /test/error3/tercer_error HTTP/1.1\" 404 234125\n",
      "64.242.88.10 - - [07/Mar/2004:17:21:59 -0800] \"ERROR /test/error4/cuarto_error HTTP/1.1\" 404 125\n"
     ]
    }
   ],
   "source": [
    "# Filtramos errores con una función\n",
    "\n",
    "def containsError(s):\n",
    "    return \"ERROR\" in s\n",
    "\n",
    "error_rdd = rdd.filter(containsError)\n",
    "for line in error_rdd.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152, 126, 98, 102, 104, 145, 102, 96, 145, 109]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map: longitud de cada línea del fichero de logs\n",
    "\n",
    "long_rdd = rdd.map(lambda x: len(x))\n",
    "long_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['64.242.88.10', '-', '-', '[07/Mar/2004:16:05:49', '-0800]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FlatMap: obtenemos las palabras del documento\n",
    "\n",
    "words_rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "words_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='key_rdd' />\n",
    "\n",
    "### Key-Value RDD\n",
    "\n",
    "Cada elemento del RDD puede contener cualquier tipo de dato: texto, números, listas, contenedores... lo único que debemos tener en cuenta es que las funciones que definimos para nuestras transformaciones sean capaces de trabajar con el dato oportuno.\n",
    "\n",
    "Existen otro tipo de transformaciones que requieren un tipo especial de RDD, los **key-value** RDD. Los datos en este tipo de RDD serán tuplas (clave-valor). Lo habitual será que mediante una transformación transformemos un RDD a un key-value RDD.\n",
    "\n",
    "Los key-value RDD además de las transformaciones ya estudiadas admiten algunas transformaciones especiales:\n",
    "\n",
    "- **keys()** retorna RDD con las claves.\n",
    "- **values()** retorna RDD con los valores.\n",
    "- **mapValues(func)** retorna un RDD en el que aplica la función definida sobre cada valor sin modificar la clave.\n",
    "- **flatMapValues(func)** retorna un RDD en el que aplica la función retornando un iterador para cada valor del RDD.\n",
    "- **reduceByKey(func)** retorna un RDD con los valores con la misma clave combinados con la función definida.\n",
    "- **groupByKey()** retorna un RDD con los valores agrupados en un iterador por la clave.\n",
    "- **sortByKey()** retorna un RDD ordenado por la clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un rdd\n",
    "\n",
    "pair_rdd = sc.parallelize([(\"B\", [4, 5]), (\"B\", [6 ,7 , 9]), (\"A\", [2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', [4, 5]), ('B', [6, 7, 9]), ('A', [2])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5], [6, 7, 9], [2]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 2), ('B', 3), ('A', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.mapValues(lambda x: len(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 4), ('B', 5), ('B', 6), ('B', 7), ('B', 9), ('A', 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', [4, 5, 6, 7, 9]), ('A', [2])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', <pyspark.resultiterable.ResultIterable at 0x7f43b4e73a50>),\n",
       " ('A', <pyspark.resultiterable.ResultIterable at 0x7f43b4e73c10>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B [[4, 5], [6, 7, 9]]\n",
      "A [[2]]\n"
     ]
    }
   ],
   "source": [
    "for pair in pair_rdd.groupByKey().collect():\n",
    "    print(pair[0], list(pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', [2]), ('B', [4, 5]), ('B', [6, 7, 9])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones sobre varios RDD:\n",
    "\n",
    "- **subtractByKey(otherRdd)** retorna un rdd borrando los elementos con la misma clave.\n",
    "- **join(otherRdd)** realiza un inner join por la clave.\n",
    "- **rightOuterJoin(otherRdd)** realiza un right outer join por la clave.\n",
    "- **leftOuterJoin(otherRdd)** realiza un left outer join por la clave.\n",
    "- **cogroup(otherRdd)** agrupa los datos de ambos rdd que comparten la misma clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', [4, 5]), ('B', [6, 7, 9])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos otro rdd\n",
    "\n",
    "other_rdd = sc.parallelize([(\"A\", [1])])\n",
    "pair_rdd.subtractByKey(other_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', ([2], [1]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.join(other_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', ([2], [1]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.rightOuterJoin(other_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', ([4, 5], None)), ('B', ([6, 7, 9], None)), ('A', ([2], [1]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.leftOuterJoin(other_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f43e4193a10>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f43e4193650>)),\n",
       " ('A',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f43e41933d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f43dc8de610>))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.cogroup(other_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B [[4, 5], [6, 7, 9]] []\n",
      "A [[2]] [[1]]\n"
     ]
    }
   ],
   "source": [
    "for pair in pair_rdd.cogroup(other_rdd).collect():\n",
    "    print(pair[0], list(pair[1][0]), list(pair[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo con el ejemplo del análisis de un web log, podríamos querer saber las palabras que más veces aparecen en los registros que contienen errores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 8), ('-0800]', 4), ('\"ERROR', 4), ('64.242.88.10', 4), ('HTTP/1.1\"', 4)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformamos en key-value RDD y sumamos las veces que aparece cada palabra\n",
    "# rdd.flatMapValues(func) -> retorna un RDD en el que aplica la función retornando un iterador para cada valor del RDD.\n",
    "# rdd.map() -> modificar elementos del rdd.\n",
    "# rdd.reduceByKey(func) -> retorna un RDD con los valores con la misma clave combinados con la función definida.\n",
    "\n",
    "kv_rdd = error_rdd.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Obtenemos las 5 primeras más usadas\n",
    "\n",
    "kv_rdd.takeOrdered(5, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='otros' />\n",
    "\n",
    "## Otros elementos de Spark Core\n",
    "\n",
    "<div id='vb' />\n",
    "\n",
    "### Variables Broadcast\n",
    "\n",
    "Son variables globales de solo lectura que se envían a todos los nodos workers de spark para que sean usadas en una o más operaciones.\n",
    "\n",
    "Un buen ejemplo sería cuando necesitamos hacer lookups sobre una tabla en la que tenemos la relación entre status code y su descripción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  Not Found  =>  4\n",
      "Status Code:  Bad Request  =>  10\n",
      "Status Code:  OK  =>  40\n"
     ]
    }
   ],
   "source": [
    "# Variables broadcast\n",
    "\n",
    "my_broadcast_var = sc.broadcast({200: \"OK\", 401: \"Bad Request\", 404: \"Not Found\"})\n",
    "\n",
    "rdd = sc.textFile(\"../data/test.log\")\n",
    "\n",
    "def extractStatusCode(line):\n",
    "    toks = line.split(' ')\n",
    "    if len(toks) > 7:\n",
    "        return (my_broadcast_var.value[int(toks[8])], 1)\n",
    "    else:\n",
    "        return (\"\", 1)\n",
    "    \n",
    "statusCodes = rdd.map(extractStatusCode).groupByKey()\n",
    "\n",
    "for line in statusCodes.collect():\n",
    "    print(\"Status Code: \", line[0], \" => \", sum(line[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='acum' />\n",
    "\n",
    "### Acumuladores\n",
    "\n",
    "Nos proporciona una sintaxis sencilla para utilizar variables globales compartidas por todos los nodos workes. Se suelen utilizar para agregar valores.\n",
    "\n",
    "Uno de los usos más comunes es contar los eventos que ocurren durante la ejecución de un trabajo. Por ejemplo, contar las líneas en blanco que tenemos en nuestro fichero de logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank lines: 0 of 54\n"
     ]
    }
   ],
   "source": [
    "# Creamos acumulador inicializado a 0\n",
    "\n",
    "blank_lines = sc.accumulator(0)\n",
    "\n",
    "def extractCallSigns(line):\n",
    "    global blank_lines\n",
    "    if (line == \"\"):\n",
    "        blak_lines += 1\n",
    "    return line\n",
    "\n",
    "call_signs = rdd.map(extractCallSigns)\n",
    "\n",
    "print(\"Blank lines: {1} of {0}\".format(call_signs.count(), blank_lines.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='ejemplo' />\n",
    "\n",
    "## Ejemplo final\n",
    "\n",
    "###### ¿Qué comunidades autónomas han realizado más contratos a mujeres que a hombres durante todo el periodo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "\n",
    "# Carga de datos\n",
    "\n",
    "#sc = SparkContext()\n",
    "cp = sc.textFile(\"../data/comunidades_provincias.csv\")\n",
    "cm = sc.textFile(\"../data/contratos_municipio.csv\")\n",
    "\n",
    "# Limpieza de datos\n",
    "\n",
    "cp_headers = cp.first()\n",
    "cp = cp.filter(lambda line: line != cp_headers)\n",
    "headers = cm.first()\n",
    "cm = cm.filter(lambda line: line != headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['201601;Almeria;Abla;27;20;7',\n",
       " '201601;Almeria;Abrucena;39;26;13',\n",
       " '201601;Almeria;Adra;456;259;197',\n",
       " '201601;Almeria;Albanchez;7;4;3']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['201601', 'Almeria', 'Abla', '27', '20', '7'],\n",
       " ['201601', 'Almeria', 'Abrucena', '39', '26', '13'],\n",
       " ['201601', 'Almeria', 'Adra', '456', '259', '197'],\n",
       " ['201601', 'Almeria', 'Albanchez', '7', '4', '3']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizamos datos\n",
    "\n",
    "def parseLine(line):\n",
    "    tokens = zip(line.split(\";\"), headers)\n",
    "    parsed_tokens = []\n",
    "    for token in tokens:\n",
    "        token_type = token[1]\n",
    "        print(\"token_type = \", token[0])\n",
    "        parsed_tokens.append(token[0])\n",
    "    return parsed_tokens\n",
    "\n",
    "cp = cp.map(parseLine)\n",
    "cm = cm.map(parseLine)\n",
    "cm.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos una prueba para transformar el número de contratos de string a entero.\n",
    "\n",
    "int(cm.take(4)[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos datos para hombres y mujeres\n",
    "\n",
    "cmh = cm.map(lambda x: (x[1], int(x[4])))\n",
    "cmm = cm.map(lambda x: (x[1], int(x[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Almeria', 20), ('Almeria', 26), ('Almeria', 259)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmh.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Almeria', 7), ('Almeria', 13), ('Almeria', 197)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmm.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos que tienen la misma key\n",
    "\n",
    "cmm.keys().collect() == cmh.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Almeria [97074] [71785]\n",
      "Cadiz [231294] [136023]\n",
      "Granada [165456] [119271]\n",
      "Sevilla [335311] [239878]\n",
      "Zaragoza [135031] [107537]\n",
      "Asturias [98137] [94136]\n",
      "Balears, Illes [171866] [144575]\n",
      "Palmas, Las [124122] [107948]\n",
      "Santa Cruz de Tenerife [101463] [93312]\n",
      "�vila [15159] [14343]\n",
      "Palencia [25083] [21602]\n",
      "Segovia [19806] [17367]\n",
      "Soria [10440] [9176]\n",
      "Cuenca [27935] [18387]\n",
      "Barcelona [666605] [620722]\n",
      "Girona [91887] [78521]\n",
      "Castellon/Castello [71152] [50250]\n",
      "Caceres [56121] [42515]\n",
      "Bizkaia [136386] [138408]\n",
      "Cordoba [173253] [104537]\n",
      "Huelva [137061] [113121]\n",
      "Jaen [171076] [81942]\n",
      "Malaga [235880] [180633]\n",
      "Huesca [32880] [24680]\n",
      "Teruel [14237] [10401]\n",
      "Cantabria [67918] [67813]\n",
      "Burgos [39792] [34683]\n",
      "Leon [37028] [36131]\n",
      "Salamanca [33322] [30722]\n",
      "Valladolid [61531] [55611]\n",
      "Zamora [14996] [14638]\n",
      "Albacete [67885] [41392]\n",
      "Ciudad Real [61271] [37881]\n",
      "Guadalajara [42710] [28730]\n",
      "Toledo [76255] [48829]\n",
      "Lleida [60744] [40048]\n",
      "Tarragona [96623] [82205]\n",
      "Alicante/Alacant [200290] [156573]\n",
      "Valencia/Valencia [336626] [229757]\n",
      "Badajoz [157336] [93751]\n",
      "Coruna, A [112484] [107807]\n",
      "Lugo [29579] [27811]\n",
      "Ourense [21061] [19223]\n",
      "Pontevedra [114930] [96059]\n",
      "Madrid [710875] [618013]\n",
      "Murcia [378124] [180291]\n",
      "Navarra [97782] [104844]\n",
      "Araba/Alava [51694] [41525]\n",
      "Gipuzkoa [71690] [81377]\n",
      "Rioja, La [43421] [36532]\n",
      "Ceuta [5005] [4084]\n",
      "Melilla [5188] [5708]\n"
     ]
    }
   ],
   "source": [
    "# Sumamos la cantidad de contratos de hombres y mujeres por provincia\n",
    "\n",
    "fh = cmh.reduceByKey(lambda x,y: x+y)\n",
    "fm = cmm.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Unimos los datos por provincia\n",
    "\n",
    "final = fh.cogroup(fm).collect()\n",
    "prov = []\n",
    "for pair in final:\n",
    "    print(pair[0], list(pair[1][0]), list(pair[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Andalucia', 'Almeria'],\n",
       " ['Andalucia', 'Cadiz'],\n",
       " ['Andalucia', 'Cordoba'],\n",
       " ['Andalucia', 'Granada']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos cómo son los datos de comunidades\n",
    "\n",
    "cp.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bizkaia [136386] [138408]\n",
      "Navarra [97782] [104844]\n",
      "Gipuzkoa [71690] [81377]\n",
      "Melilla [5188] [5708]\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos las provincias con mayor contratos de mujeres\n",
    "\n",
    "provincia = []\n",
    "\n",
    "for pair in final:\n",
    "    if list(pair[1][0]) < list(pair[1][1]):\n",
    "        print(pair[0], list(pair[1][0]), list(pair[1][1]))\n",
    "        provincia.append(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Navarra, Comunidad Foral de', 'Melilla', 'Pais Vasco']\n"
     ]
    }
   ],
   "source": [
    "# Finalmente seleccionamos las comunidades de dichas provincias\n",
    "\n",
    "comunidad = []\n",
    "\n",
    "for pair in cp.groupByKey().collect():\n",
    "    for p in provincia:\n",
    "        if p in list(pair[1]):\n",
    "            comunidad.append(pair[0])\n",
    "            \n",
    "comunidad = list(set(comunidad))\n",
    "print(comunidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desde el principio con los contratos en la misma rdd.\n",
    "\n",
    "rdd = cm.map(lambda x: (x[1], (int(x[4]), int(x[5]))))\n",
    "\n",
    "# Como puedo hacer la suma de contratos por género??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 7), (26, 13), (259, 197)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.values().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Almeria', 'Almeria', 'Almeria']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keys().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Almeria', <pyspark.resultiterable.ResultIterable at 0x7f43dc88da10>),\n",
       " ('Cadiz', <pyspark.resultiterable.ResultIterable at 0x7f43dc8d0910>),\n",
       " ('Granada', <pyspark.resultiterable.ResultIterable at 0x7f43dc8d0d10>),\n",
       " ('Sevilla', <pyspark.resultiterable.ResultIterable at 0x7f43dc8d0410>),\n",
       " ('Zaragoza', <pyspark.resultiterable.ResultIterable at 0x7f43dc8791d0>),\n",
       " ('Asturias', <pyspark.resultiterable.ResultIterable at 0x7f43dc87e550>),\n",
       " ('Balears, Illes', <pyspark.resultiterable.ResultIterable at 0x7f43b4e8d190>),\n",
       " ('Palmas, Las', <pyspark.resultiterable.ResultIterable at 0x7f43dc8d0f50>),\n",
       " ('Santa Cruz de Tenerife',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f43b4e3cad0>),\n",
       " ('�vila', <pyspark.resultiterable.ResultIterable at 0x7f43b4e3c950>),\n",
       " ('Palencia', <pyspark.resultiterable.ResultIterable at 0x7f43b4e73c50>),\n",
       " ('Segovia', <pyspark.resultiterable.ResultIterable at 0x7f43dc879690>),\n",
       " ('Soria', <pyspark.resultiterable.ResultIterable at 0x7f43dc879590>),\n",
       " ('Cuenca', <pyspark.resultiterable.ResultIterable at 0x7f43dc8795d0>),\n",
       " ('Barcelona', <pyspark.resultiterable.ResultIterable at 0x7f43dc879390>),\n",
       " ('Girona', <pyspark.resultiterable.ResultIterable at 0x7f43dc8d0e10>),\n",
       " ('Castellon/Castello',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f43dc879a50>),\n",
       " ('Caceres', <pyspark.resultiterable.ResultIterable at 0x7f43dc879210>),\n",
       " ('Bizkaia', <pyspark.resultiterable.ResultIterable at 0x7f43dc879290>),\n",
       " ('Cordoba', <pyspark.resultiterable.ResultIterable at 0x7f43dc8d0350>),\n",
       " ('Huelva', <pyspark.resultiterable.ResultIterable at 0x7f43dc879f90>),\n",
       " ('Jaen', <pyspark.resultiterable.ResultIterable at 0x7f43dc8790d0>),\n",
       " ('Malaga', <pyspark.resultiterable.ResultIterable at 0x7f43dc879950>),\n",
       " ('Huesca', <pyspark.resultiterable.ResultIterable at 0x7f43dc879b90>),\n",
       " ('Teruel', <pyspark.resultiterable.ResultIterable at 0x7f43dc8cfc90>),\n",
       " ('Cantabria', <pyspark.resultiterable.ResultIterable at 0x7f43dc8cf290>),\n",
       " ('Burgos', <pyspark.resultiterable.ResultIterable at 0x7f43dc879850>),\n",
       " ('Leon', <pyspark.resultiterable.ResultIterable at 0x7f43dc8cf8d0>),\n",
       " ('Salamanca', <pyspark.resultiterable.ResultIterable at 0x7f43dc8b9d10>),\n",
       " ('Valladolid', <pyspark.resultiterable.ResultIterable at 0x7f43dc875c10>),\n",
       " ('Zamora', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c12d0>),\n",
       " ('Albacete', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c65d0>),\n",
       " ('Ciudad Real', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6bd0>),\n",
       " ('Guadalajara', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6c90>),\n",
       " ('Toledo', <pyspark.resultiterable.ResultIterable at 0x7f43b4e3cc90>),\n",
       " ('Lleida', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6c10>),\n",
       " ('Tarragona', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6b50>),\n",
       " ('Alicante/Alacant',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6250>),\n",
       " ('Valencia/Valencia',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6690>),\n",
       " ('Badajoz', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6d50>),\n",
       " ('Coruna, A', <pyspark.resultiterable.ResultIterable at 0x7f43dc889950>),\n",
       " ('Lugo', <pyspark.resultiterable.ResultIterable at 0x7f43dc889610>),\n",
       " ('Ourense', <pyspark.resultiterable.ResultIterable at 0x7f43dc8c6190>),\n",
       " ('Pontevedra', <pyspark.resultiterable.ResultIterable at 0x7f43dc8892d0>),\n",
       " ('Madrid', <pyspark.resultiterable.ResultIterable at 0x7f43dc889b10>),\n",
       " ('Murcia', <pyspark.resultiterable.ResultIterable at 0x7f43dc889090>),\n",
       " ('Navarra', <pyspark.resultiterable.ResultIterable at 0x7f43dc889410>),\n",
       " ('Araba/Alava', <pyspark.resultiterable.ResultIterable at 0x7f43dc8891d0>),\n",
       " ('Gipuzkoa', <pyspark.resultiterable.ResultIterable at 0x7f43dc8890d0>),\n",
       " ('Rioja, La', <pyspark.resultiterable.ResultIterable at 0x7f43dc889350>),\n",
       " ('Ceuta', <pyspark.resultiterable.ResultIterable at 0x7f43dc879ed0>),\n",
       " ('Melilla', <pyspark.resultiterable.ResultIterable at 0x7f43dc889190>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
