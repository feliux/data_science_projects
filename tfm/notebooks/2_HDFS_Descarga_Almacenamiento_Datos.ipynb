{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data y Machine Learning para clasificación de galaxias\n",
    "\n",
    "- [Descarga de datos](#data)\n",
    "    - [Dataset de parámetros e instrumentación](#data1)\n",
    "    - [Dataset GalaxyZoo](#data2)\n",
    "    - [Dataset de imágenes](#data3)\n",
    "- [Almacenamiento HDFS](#hdfs)\n",
    "\n",
    "    \n",
    "<div id='xx' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='data' />\n",
    "\n",
    "## Descarga de datos\n",
    "\n",
    "Primero que nada debemos descargar nuestro datos en el contenedor y, posteriormente, almacenarlos en HDFS.\n",
    "\n",
    "Descargaremos varios ficheros `.csv` que consisten en un sub-conjunto de datos de la web del proyecto [SDSS](https://www.sdss.org/) almacenados en un repositorio de la UAB. En concreto descargaremos los datos relativos a la *Data Realease 14*: [link1](https://www.sdss.org/dr14/) y [link2](http://skyserver.sdss.org/dr14/en/tools/toolshome.aspx). Éstos datos contendrán información delativa al brillo del objeto, características del telescopio, etc.\n",
    "\n",
    "Las imágenes de galaxias tomadas para este proyecto son descargadas como imágenes `png` de 64x64 píxeles en color en base a un script para luego ser procesadas y almacenadas en un fichero.\n",
    "\n",
    "- [SDSS DR14 Finding Chart Tool](https://skyserver.sdss.org/dr14/en/tools/chart/chartinfo.aspx)\n",
    "- [SDSS Science Archive Server (SAS)](https://dr14.sdss.org/comingsoon/imaging/field/search)\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/galaxias.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Las imágenes que descargaremos serán aquellas que corresponden al dataset del proyecto *GalaxyZoo* y por lo tanto tendremos la certeza de que contamos con una clasificación del tipo de galaxia.\n",
    "\n",
    "<div id='data1' />\n",
    "\n",
    "#### Dataset: parámetros e instrumentación\n",
    "\n",
    "Mediante estas sencillas instrucciones descargaremos los datos (trabajaremos dentro del directorio `/home/cloudera/`):\n",
    "\n",
    "`\n",
    "[container]$ mkdir -p /home/cloudera/galaxias/data\n",
    "[container]$ cd /home/cloudera/galaxias/data\n",
    "[container]$ wget https://nebula.uab.cat/share/proxy/alfresco-noauth/api/internal/shared/node/7mRDxMhySJu0OMG0drr1MA/content/SDSS_parametros.zip\n",
    "[container]$ unzip SDSS_parametros.zip\n",
    "[container]$ tree /home/cloudera/galaxias/data\n",
    "`\n",
    "\n",
    "~~~\n",
    "galaxias/data/\n",
    "├── SDSS_PhotObj.csv\n",
    "└── SDSS_SpecObj.csv\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "<div id='data2' />\n",
    "\n",
    "#### Dataset: GalaxyZoo\n",
    "\n",
    "El [dataset](https://data.galaxyzoo.org/) de GalaxyZoo representa el conjunto de datos principal de este proyecto por dos motivos. En primer lugar es el que contiene la correlación entre el `id` del objeto (nuestra imagen) y la clasificación de los usuarios. Y en segundo lugar es el que utilizaremos para seleccionar las imágenes que vamos a descargar.\n",
    "\n",
    "`\n",
    "[container]$ cd /home/cloudera/galaxias/data\n",
    "[container]$ wget http://galaxy-zoo-1.s3.amazonaws.com/GalaxyZoo1_DR_table2.csv.zip\n",
    "[container]$ unzip GalaxyZoo1_DR_table2.csv.zip\n",
    "[container]$ tree /home/cloudera/galaxias/data\n",
    "`\n",
    "\n",
    "~~~\n",
    "galaxias/data/\n",
    "├── GalaxyZoo1_DR_table2.csv\n",
    "├── SDSS_PhotObj.csv\n",
    "└── SDSS_SpecObj.csv\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "<div id='data3' />\n",
    "\n",
    "#### Dataset: conjunto de imágenes\n",
    "\n",
    "Desarrollaremos un script para la descarga de imágenes que posteriormente utilizaremos para entrenar nuestros modelos de clasificación.\n",
    "\n",
    "La mayoría de algoritmos de machine learning necesitan que los datos estén organizados de forma tabular, con las observaciones (imágenes en nuestro caso) distribuidas por filas y los parámetros (píxeles) de cada observación en columnas. Así pues, convertiremos las imágenes en un vector de `64x64 = 4096` posiciones en base a los siguientes pasos:\n",
    "\n",
    "- Convertir las imágenes en color a una imagen en blanco y negro. Cada uno de los píxeles de la imagen contiene 3 valores entre 0 y 255 correspondientes a los 3 canales RGB (R=Rojo, G=Verde, B=azul). Podríamos decir que cada imagen está compuesta por tres matrices `64x64: MR, MG, MB` correspondientes a los canales R, G, B respectivamente. Aplicaremos la siguiente formula para fusionar las tres matrices en una sola matriz en tono de grises: `M = 0.2989 * MR + 0.5870 * MG + 0.1140 * MB`\n",
    "\n",
    "\n",
    "- Aplanamos la matriz `M` a un vector, esto es, reorganizamos los valores de la matriz `M` de dimensiones `64x64` a un vetor de longitud `4096`.\n",
    "\n",
    "\n",
    "- Algunos algoritmos funcionan mejor si todas las variables se encuentran dentro del mismo rango de valores, típicamente `[-1, 1] o [0, 1]`. Por ello normalizamos los valores del vector dividiendolos entre 255.\n",
    "\n",
    "\n",
    "- Para poder entrenar un algoritmo de clasificación también necesitaremos los *targets* de las observaciones. Éstas se obtienen de las clasificaciones del dataset de GalaxyZoo y tienen los siguientes valores:\n",
    "\n",
    "\n",
    "    - target = 0 -> clase = incierto\n",
    "    - target = 1 -> clase = elíptica\n",
    "    - target = 2 -> clase = espiral\n",
    "    \n",
    "- Juntamos los dos conjuntos de datos en un solo fichero que contiene tanto los *features* como el *target*. Este conjunto de datos tiene los campos:\n",
    "\n",
    "\n",
    "    - drobjid: identificador del objeto.\n",
    "    - target: clase del objeto.\n",
    "    - F0 a F4095: correspondiente al vector de 4095 atributos de la imagen.\n",
    "    \n",
    "El desarrollo de este apartado la llevaremos a cabo en la segunda parte de este trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hdfs' />\n",
    "\n",
    "## Almacenamiento HDFS\n",
    "\n",
    "Para comunicarse con el sistema distribuido de ficheros de HADOOP (HDFS), existen una serie de comandos de shell proporcionados por el mismo sistema. Lo primero que vamos a hacer es crear un directorio en el HDFS para cada uno de los ficheros que posteriormente vamos a analizar. Para la creación de un directorio en HDFS utilizamos el siguiente comando:\n",
    "\n",
    "---\n",
    "\n",
    "`[container]$ hdfs dfs -mkdir /user/cloudera/galaxias\n",
    " mkdir: Permission denied: user=root, access=WRITE, inode=\"/user/cloudera\":cloudera:cloudera:drwxr-xr-x\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "Vemos que no podemos escribir sobre ese destino en el HDFS como usuario 'root' ya que pertenece al usuario 'cloudera'. Además no es muy aconsejable trabajar con 'root' en HDFS. En definitiva cambiamos a usuario 'impala' y nuestro directorio destino (posteriormente veremos el motivo de esta elección):\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ su impala\n",
    "[container]$ pwd\n",
    "/home/cloudera/galaxias\n",
    "[container]$ hdfs dfs -mkdir /user/hive/galaxias\n",
    "[container]$ hdfs dfs -ls /user/hive\n",
    "Found 2 items\n",
    "drwxr-xr-x   - impala supergroup          0 2020-05-23 13:25 /user/hive/galaxias\n",
    "drwxrwxrwx   - hive   supergroup          0 2020-05-22 22:07 /user/hive/warehouse\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "Creamos un directorio para cada uno de los archivos de datos dentro de HDFS de igual manera que hicimos en el caso anterior:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -mkdir /user/hive/galaxias/SDSS_PhotObj\n",
    "[container]$ hdfs dfs -mkdir /user/hive/galaxias/SDSS_SpecObj\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "El motivo de haber creado un directorio para cada fichero se explicará más adelante a la hora de modelizar los datos.\n",
    "\n",
    "A continuación, para realizar la carga o ingestión de los datos desde el directorio local hasta el directorio que acabamos de crear en el HDFS usamos el comando `put`:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -put data/SDSS_PhotObj.csv /user/hive/galaxias/SDSS_PhotObj/SDSS_PhotObj.csv\n",
    "[container]$ hdfs dfs -put data/SDSS_SpecObj.csv /user/hive/galaxias/SDSS_SpecObj/SDSS_SpecObj.csv\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "Y podemos comprobar que ambos ficheros se han creado correctamente:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -ls /user/hive/galaxias/SDSS_PhotObj/\n",
    "Found 1 items\n",
    "-rw-r--r--   1 impala supergroup  1248036 /user/hive/galaxias/SDSS_PhotObj/SDSS_PhotObj.csv\n",
    "[container]$ hdfs dfs -ls /user/hive/galaxias/SDSS_SpecObj/\n",
    "Found 1 items\n",
    "-rw-r--r--   1 impala supergroup  1248036 /user/hive/galaxias/SDSS_SpecObj/SDSS_SpecObj.csv\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "Esto mismo lo podemos comprobar accediendo al NODEMANAGER vía web a través de la url http://localhost:50070 y seleccionando *'Browse the file system'*. Veamos un ejemplo:\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/nodemanager_1.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Navegamos dentro del directorio de archivos hasta encontrar nuestros datos recién almacenados.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/nodemanager_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para copiar algún fichero desde el HDFS al directorio local de nuestro contenedor entonces tendríamos que usar el comando `get`, por ejemplo:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -get /user/hive/galaxias/SDSS_SpecObj/SDSS_SpecObj.csv ~/galaxias/data/SDSS_SpecObj.csv\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "Podemos ver el tamaño de los archivos transferidos a HDFS con el siguiente comando:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -count -h /user/hive/galaxias/SDSS_PhotObj/SDSS_PhotObj.csv\n",
    "           0            1              2.8 M /user/hive/galaxias/SDSS_PhotObj/SDSS_PhotObj.csv\n",
    "[container]$ hdfs dfs -count -h /user/hive/galaxias/SDSS_SpecObj/SDSS_SpecObj.csv\n",
    "           0            1              1.2 M /user/hive/galaxias/SDSS_SpecObj/SDSS_SpecObj.csv\n",
    "`\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
