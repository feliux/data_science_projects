{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data y Machine Learning para clasificación de galaxias\n",
    "\n",
    "- [Almacenamiento HIVE - HUE - IMPALA](#hue-impala)\n",
    "    - [Dataset SDSS_PhotObj](#phot)\n",
    "    - [Dataset SDSS_SpecObj](#spec)\n",
    "    - [Modelo relacional](#relacional)\n",
    "        - [HIVE: External Table](#hive-external)\n",
    "        - [HIVE: Optimized Row Columnar](#hive-orc)\n",
    "    - [Administración HUE](#hue)\n",
    "        - [HUE Queries](#hue-queries)\n",
    "        - [HUE: Optimized Row Columnar](#hue-orc)\n",
    "        - [HUE: Filtrado de datos](#hue-filtrado)\n",
    "    - [Analítica HIVE - SPARK](#hive-spark)\n",
    "\n",
    "\n",
    "<div id='xx' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hue-impala' />\n",
    "\n",
    "## Almacenamiento HIVE - HUE - IMPALA\n",
    "\n",
    "Para un buen manejo de los datos y su posterior análisis es fundamental que hayamos definido un “correcto” modelo de datos. En esta sección vamos a explicar lo que es el modelo de datos relacional, implementaremos el modelo usando una de las herramientas del ecosistema de HADOOP llamada HIVE y, finalmente, sobre el modelo ya definido ejecutaremos una serie de consultas y análisis sencillos sobre los datos usando herramientas como HUE e IMPALA.\n",
    "\n",
    "En primer lugar comprobamos que los datos transferidos en la sección anterior se han copiado correctamente:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -cat /user/hive/galaxias/SDSS_PhotObj/SDSS_PhotObj.csv | wc -l\n",
    "25101\n",
    "[container]$ hdfs dfs -cat /user/hive/galaxias/SDSS_SpecObj/SDSS_SpecObj.csv | wc -l\n",
    "25101\n",
    "[container]$ hdfs dfs -cat /user/hive/galaxias/SDSS_PhotObj/SDSS_PhotObj.csv | head -n1\n",
    "objid,fileid,ra,dec,u,g,r,i,z,field\n",
    "[container]$ hdfs dfs -cat /user/hive/galaxias/SDSS_SpecObj/SDSS_SpecObj.csv | head -n1\n",
    "objid,redshift,plate,mjd,fiberid,class\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "Para poder acceder a la información que contiene estos ficheros de texto es necesario definir su modelo de datos. En primer lugar notamos que los ficheros *SDSS_PhotObj.csv y SDSS_SpecObj.csv* contienen 25100 filas de observaciones. Además podemos ver las variables de nuestro datos en la cabecera del archivo .csv, los cuales pasamos a detallar a continuación:\n",
    "\n",
    "<div id='phot' />\n",
    "\n",
    "##### SDSS_PhotObj.csv\n",
    "\n",
    "- **objid (int)** \n",
    "    - identificador único del objeto en el catálogo de SDSSDR8.\n",
    "- **fileid (int)**\n",
    "    - alias del parámetro “dr7objid”, el identificador que usamos en este proyecto para el fichero que contiene la imagen correspondiente del objeto.\n",
    "- **ra, dec (float)**\n",
    "    - coordenadas celestes del objeto ('right ascension' y 'declination', en grados).\n",
    "- **u,g,r,i y z (float)**\n",
    "    - brillo (magnitud) medido en los diferentes filtros de colores por cada objeto. Las letras son nombres estándares de filtros que dejan pasar solo luz en un determinado intervalo de longitudes de onda. Cuanto menor es este valor, más brillante es el objeto.\n",
    "- **field (int)**\n",
    "    - número que identifica el área de cielo en la que seencuentra el objeto.\n",
    "    \n",
    "Si quisiéramos obtener las coordenadas celestes para la última observación (fila) de nuestro dataset de manera rápida podríamos lanzar el siguiente comando sobre HDFS:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -cat /user/cloudera/galaxias/SDSS_PhotObj/SDSS_PhotObj.csv | tail -n1 | cut -f \"3,4\" -d \",\"\n",
    "120.93984795,44.83061183\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "Nuestro segundo dataset contiene los siguientes tipos de datos:\n",
    "\n",
    "<div id='spec' />\n",
    "\n",
    "##### SDSS_SpecObj.csv\n",
    "\n",
    "- **objid (int)**\n",
    "    - identificador único de la galaxia (objeto) en el catálogo SDSS.\n",
    "- **redshift (float)**\n",
    "    - estimación de corrimiento al rojo (alias 'Z'). Se trata de un indicador de la distancia del objeto.\n",
    "- **plate, fiberid (int)**\n",
    "    - identificadores de la instrumentación del telescopio que se ha utilizado para hacer la medida que ha llevado a la estimación de redshift.\n",
    "- **mjd (int)**\n",
    "    - identificador de la fecha en la que se ha tomado la medida (modified Julian date).\n",
    "- **class (string)**\n",
    "    - nombre de la clase del objeto, según las características de la luz emitida en el espectro electromagnético observado por SDSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='relacional' />\n",
    "\n",
    "### Modelo relacional\n",
    "\n",
    "En nuestro caso vamos a almacenar nuestros datos en una base de datos relacional. En este tipo de modelo la información está organizada en tablas. Estas tablas están compuestas por registros (cada fila de la tabla sería un registro) y columnas (también llamadas campos). El modelo de datos relacional es el más utilizado en la actualidad para modelar problemas reales y administrar datos dinámicamente.\n",
    "\n",
    "HIVE es un sistema de almacenamiento de datos construido sobre HADOOP para proporcionar agrupación, consulta y análisis de datos masivos. HIVE define un tipo especial de tabla, llamada *tabla externa*, que relaciona los campos de la propia tabla con los campos de un fichero. Por otro lado, IMPALA es un motor de consultas SQL open source para el procesamiento masivo de datos en paralelo.\n",
    "\n",
    "Primero que nada veamos como interaccionar con HIVE y crear una base de datos:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ sudo su\n",
    "[container]$ hive\n",
    "[hive]$ show databases;\n",
    "OK\n",
    "default\n",
    "Time taken: 70.633 seconds, Fetched: 1 row(s)\n",
    "[hive]$ create database galaxias;\n",
    "OK\n",
    "Time taken: 0.086 seconds\n",
    "[hive]$ show databases;\n",
    "OK\n",
    "default\n",
    "galaxias\n",
    "Time taken: 0.012 seconds, Fetched: 2 row(s)\n",
    "[hive]$ use galaxias;\n",
    "OK\n",
    "Time taken: 0.021 seconds\n",
    "`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hive-external' />\n",
    "\n",
    "#### HIVE: External Table\n",
    "\n",
    "Fijándonos en la sección anterior donde se definen los nombres de los campos y sus tipos, definimos la tabla externa correspondiente al fichero *SDSS_PhotObj.csv*:\n",
    "\n",
    "---\n",
    "\n",
    "~~~\n",
    "CREATE EXTERNAL TABLE sdss_photobj_csv (\n",
    "    objid BIGINT COMMENT 'identificador unico del objeto en el catalogo deSDSS',\n",
    "    fileid BIGINT COMMENT 'alias del parametro \\\"dr7objid\\\", identificador que usamos en este proyecto para el fichero que contiene la imagen correspondiente del objeto',\n",
    "    ra FLOAT COMMENT 'ascension recta (grados)',\n",
    "    dec FLOAT COMMENT 'declinacion (grados)',\n",
    "    u FLOAT COMMENT 'brillo (magnitud) medido en el filtro u',\n",
    "    g FLOAT COMMENT 'brillo (magnitud) medido en el filtro g',\n",
    "    r FLOAT COMMENT 'brillo (magnitud) medido en el filtro r',\n",
    "    i FLOAT COMMENT 'brillo (magnitud) medido en el filtro i',\n",
    "    z FLOAT COMMENT 'brillo (magnitud) medido en el filtro z',\n",
    "    field INT COMMENT 'Numero que identifica el area de cielo en la que se encuentra el objeto')\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/hive/galaxias/SDSS_PhotObj'\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "En primer lugar vemos que la tabla se define con el comando \"CREATE EXTERNAL TABLE\" y a continuación aparece el nombre de la tabla en minúsculas con el sufijo \"\\_csv\" para indicarnos que se trata de datos csv. A continuación y entre paréntesis aparece la lista de columnas con sus tipos y comentarios. Finalmente se especifica el formato con el que hemos guardado los datos (csv separados por comas) y la ubicación de los mismos.\n",
    "\n",
    "Podemos construir la tabla para el fichero *SDSS_SpecObj.csv* con la siguiente sentencia:\n",
    "\n",
    "---\n",
    "\n",
    "~~~\n",
    "CREATE EXTERNAL TABLE sdss_specobj_csv (\n",
    "    objid BIGINT COMMENT 'identificador unico del objeto en el catalogo deSDSS',    \n",
    "    redshift FLOAT COMMENT 'estimacion de corrimiento al rojo, como indicador de la distancia del objeto, se suele utilizar la letra \"z\" como alias',    \n",
    "    plate INT COMMENT 'identificador de la instrumentacion del telescopio que se ha utilizado para hacer la medida que ha llevado a la estimacion de redshift',    \n",
    "    mjd INT COMMENT 'identificador de la fecha en la que se ha tomado la medida (viene del ingles modified Julian date)',    \n",
    "    fiberid INT COMMENT 'identificador de la instrumentacion del telescopio que se ha utilizado para hacer la medida que ha llevado a la estimacion de redshift',    \n",
    "    class STRING COMMENT 'nombre de la clase del objeto segun sus caracteristicas espectrales')\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/hive/galaxias/SDSS_SpecObj'\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "Y para comprobar que las tablas han sido creadas, fuera de la consola de HIVE ejecutamos:\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "[container]$ hdfs dfs -ls /user/hive/warehouse/galaxias.db/sdss_photobj_csv\n",
    "Found 1 items\n",
    "-rw-r--r--   1 impala supergroup  2928399 /user/hive/warehouse/galaxias.db/sdss_photobj_csv/SDSS_PhotObj.csv\n",
    "[container]$ hdfs dfs -ls /user/hive/warehouse/galaxias.db/sdss_specobj_csv\n",
    "Found 1 items\n",
    "-rw-r--r--   1 impala supergroup  1248036 /user/hive/warehouse/galaxias.db/sdss_specobj_csv/SDSS_SpecObj.csv\n",
    "`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hive-orc' />\n",
    "\n",
    "#### HIVE: Optimized Row Columnar\n",
    "\n",
    "Hasta ahora hemos definido el concepto de tabla externa en HIVE como una tabla que relaciona campos de la propia tabla con los campos de un fichero. Y hemos comprobado que podemos acceder a los datos del fichero a través del intérprete HIVE. Sin embargo podemos definir otros tipos de tablas en HIVE. La diferencia entre las tablas externas y las tablas \"normales\" es que si ejecutamos el comando \"DROP TABLE\" y se trata de una tabla externa, el fichero a los que apunta la tabla **no** se borrarán. Por el contrario si borramos una tabla \"normal\", los datos que se encuentran dentro de la tabla **sí** que se eliminarán. Creamos tablas \"normales\" porque suelen ser mucho más rápidas para realizar análisis sobre los datos en comparación con las tablas externas.\n",
    "\n",
    "A continuación mostramos la sentencia para crear este nuevo tipo de tablas para el conjunto de datos *SDSS_PhotObj.csv* y *SDSS_SpecObj.csv*:\n",
    "\n",
    "---\n",
    "\n",
    "~~~\n",
    "CREATE TABLE sdss_photobj (\n",
    "    objid BIGINT COMMENT 'identificador unico del objeto en el catalogo de SDSS',\n",
    "    fileid BIGINT COMMENT 'alias del parametro \\\"dr7objid\\\", identificador que usamos en este proyecto para el fichero que contiene la imagen correspondiente del objeto',\n",
    "    ra FLOAT COMMENT 'ascension recta (grados)',\n",
    "    dec FLOAT COMMENT 'declinacion (grados)',\n",
    "    u FLOAT COMMENT 'brillo (magnitud) medido en el filtro u',\n",
    "    g FLOAT COMMENT 'brillo (magnitud) medido en el filtro g',\n",
    "    r FLOAT COMMENT 'brillo (magnitud) medido en el filtro r',\n",
    "    i FLOAT COMMENT 'brillo (magnitud) medido en el filtro i',\n",
    "    z FLOAT COMMENT 'brillo (magnitud) medido en el filtro z',\n",
    "    field INT COMMENT 'Numero que identifica el area de cielo en la que se encuentra el objeto')\n",
    "STORED AS ORC;\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "~~~\n",
    "CREATE TABLE sdss_specobj (\n",
    "    objid BIGINT COMMENT 'identificador unico del objeto en el catalogo deSDSS',    \n",
    "    redshift FLOAT COMMENT 'estimacion de corrimiento al rojo, como indicador de la distancia del objeto, se suele utilizar la letra \"z\" como alias',    \n",
    "    plate INT COMMENT 'identificador de la instrumentacion del telescopio que se ha utilizado para hacer la medida que ha llevado a la estimacion de redshift',    \n",
    "    mjd INT COMMENT 'identificador de la fecha en la que se ha tomado la medida (viene del ingles modified Julian date)',    \n",
    "    fiberid INT COMMENT 'identificador de la instrumentacion del telescopio que se ha utilizado para hacer la medida que ha llevado a la estimacion de redshift',    \n",
    "    class STRING COMMENT 'nombre de la clase del objeto segun sus caracteristicas espectrales')\n",
    "STORED AS ORC;\n",
    "~~~\n",
    "\n",
    "---\n",
    "\n",
    "Las diferencias fundamentales entre la definición de la tabla externa y ésta es que no aparece el comando \"EXTERNAL\", y que el formato con el que la guardamos en este caso es el ORC (Optimized Row Columnar). El formato ORC es un formato eficiente para guardar datos en HIVE, y está diseñado para superar distintas limitaciones de otros formatos, además de mejorar la eficiencia en la lectura, escritura y análisis de los datos. Hay que tener en cuenta que el comando únicamente define la tabla pero que ésta todavía está vacía. Insertaremos los datos en la tabla en las próximas secciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hue' />\n",
    "\n",
    "### Administración con HUE\n",
    "\n",
    "Hue es una herramienta con interfaz de usuario web para la gestión de HADOOP, además de una plataforma para construir aplicaciones a medida sobre esta librería UI. Podemos usar esta herramienta para crear las tablas en HIVE y hacer consultas sobre ellas. Veamos como hacerlo.\n",
    "\n",
    "El puerto desde el que accedemos a HUE varía en función del mapeo que hayamos hecho al arrancar nuestro contenedor, pero por defecto se encuentra en el puerto 8888. En nuestro caso nos dirigimos desde nuestro navegador a la dirección http://localhost:8888.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_1.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Navegamos hasta nuestra base de datos 'galaxias' y creamos la tabla 'sdss_photobj_csv':\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_2.png\">\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Seleccionamos la estructura y localización en HDFS de nuestro fichero de datos\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_3.png\">\n",
    "\n",
    "<img src=\"../images/hue_4.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Para terminar repetimos el mismo proceso para el conjunto *SDSS_SpecObj.csv*.\n",
    "\n",
    "Previamente habíamos seleccionado el usuario 'impala' para transferir los archivos a HDFS. Debemos trabajar con el usuario adecuado para no estrar en conflictos de propietarios de archivos. La manupilación de los datos en HDFS mediante HUE son realizadas por el usuario 'impala'. En la siguiente imagen vemos un error de creación de tabla debido a que los datos se encuentran en el directorio `/user/cloudera` cuyo propietario es el usuario 'cloudera'.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_5_error.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Si la creación de tablas ha funcionado deberíamos ver el siguiente mensaje:\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_6.png\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hue-queries' />\n",
    "\n",
    "#### HUE Queries\n",
    "\n",
    "Resulta muy intuitivo trabajar en HUE. Para ver cómo funciona podemos acceder a los datos de la tabla haciendo algunos ejemplos sencillos. Seleccionamos la pestaña 'Query' y elegimos nuestro editor preferido para hacer consultas (en nuestro caso IMPALA):\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_7.png\">\n",
    "\n",
    "<img src=\"../images/hue_8.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Para hacer comprobaciones rápidas es muy común el uso del comando LIMIT tal como muestra la sentencia de la línea 2, para que en caso de que la respuesta contenga muchas entradas, ésta no sea mostrada por pantalla.\n",
    "\n",
    "En las líneas 3 y 4 se consultan las observaciones para las cuales las variables de brillo 'u' y 'g' registran el valor -9999. En astronomía se usan estos valores para señalar que las medidas no se hicieron correctamente.\n",
    "\n",
    "En la imagen anterior, del conjunto de consultas se ha ejecutado la correspondiente a la línea 7, la cual junto con la 8 nos muestra el número de diferentes observaciones para la variable 'class'. Las líneas 5 y 6 nos mostrarían las correspondientes tablas para cada clase de observación.\n",
    "\n",
    "Las filas 9 y 10 nos muestra el número de observaciones. Con esta búsqueda constatamos que cada registro es único (no hay elementos duplicados ya que 'objid' es la clave principal de ambas tablas) y que el número de observaciones entre los distintos datasets concuerdan, siendo un total de 25100 para cada uno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hue-orc' />\n",
    "\n",
    "#### HUE: Optimized Row Columnar\n",
    "\n",
    "Crear las tablas ORC también se puede hacer de manera interactiva. Tan solo tendremos que rellenar los campos manualmente como se muestra en la siguiente imagen.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_9.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Y repetimos el proceso para el conjunto *sdss_photobj*. Hay que recordar que estas tablas permanecen vacías hasta la inserción de datos.\n",
    "\n",
    "Para finalizar, gracias a HUE podemos guardar nuestras consultas, ver estadísticas de los datos, preparar dashboards y programar workflows de manipulación de datos entre otras muchas cosas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hue-filtrado' />\n",
    "\n",
    "#### HUE: Filtrado de datos\n",
    "\n",
    "Vamos a aprovechar que convertimos los datos al formato ORC para deshacernos de aquellos que no vamos a utilizar en nuestro análisis.\n",
    "\n",
    "Sabemos que nuestro objetivo final es el de clasificar galaxias entre espirales e irregulares, así que no queremos tener estrellas de por medio. Con el siguiente comando, además de rellenar la tabla que acabamos de crear, estamos eliminando todas las entradas clasificadas como \"STAR\" (se puede ejecutar desde la consola de HIVE).\n",
    "\n",
    "---\n",
    "\n",
    "`INSERT INTO TABLE sdss_specobj SELECT * FROM sdss_specobj_csv WHERE class!='STAR';`\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_10.png\">\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a eliminar las entradas con valores problemáticos, -9999, que como ya hemos comentado identifican flujo o magnitud incorrectos o extraños. Este sería el comando con el que cargamos la tabla pero únicamente con los valores que nos interesan.\n",
    "\n",
    "---\n",
    "\n",
    "`\n",
    "INSERT INTO TABLE sdss_photobj SELECT * FROM sdss_photobj_csv\n",
    "WHERE u != -9999.0 \n",
    "AND g != -9999.0  \n",
    "AND r != -9999.0  \n",
    "AND i != -9999.0  \n",
    "AND z != -9999.0;\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../images/hue_11.png\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='hive-spark' />\n",
    "\n",
    "### Analítica HIVE - SPARK\n",
    "\n",
    "Podemos llevar a cabo un análisis exploratorio sobre los datos guardados en HIVE utilizando `pyspark`. A continuación replicamos algunos ejemplos del apartado anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "\n",
    "hive = HiveContext(sc)\n",
    "\n",
    "photobj_csv = hive.table('galaxias.sdss_photobj_csv')\n",
    "photobj = hive.table('galaxias.sdss_photobj')\n",
    "\n",
    "specobj_csv = hive.table('galaxias.sdss_specobj_csv')\n",
    "specobj = hive.table('galaxias.sdss_specobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photobj data:  DataFrame[objid: bigint, fileid: bigint, ra: float, dec: float, u: float, g: float, r: float, i: float, z: float, field: int]\n",
      "specobj data:  DataFrame[objid: bigint, redshift: float, plate: int, mjd: int, fiberid: int, class: string]\n"
     ]
    }
   ],
   "source": [
    "print(\"photobj data: \", photobj)\n",
    "print(\"specobj data: \", specobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('objid', 'bigint'),\n",
       " ('fileid', 'bigint'),\n",
       " ('ra', 'float'),\n",
       " ('dec', 'float'),\n",
       " ('u', 'float'),\n",
       " ('g', 'float'),\n",
       " ('r', 'float'),\n",
       " ('i', 'float'),\n",
       " ('z', 'float'),\n",
       " ('field', 'int')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photobj.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('objid', 'bigint'),\n",
       " ('redshift', 'float'),\n",
       " ('plate', 'int'),\n",
       " ('mjd', 'int'),\n",
       " ('fiberid', 'int'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specobj.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+----------+---------+---------+---------+---------+---------+-----+\n",
      "|              objid|            fileid|       ra|       dec|        u|        g|        r|        i|        z|field|\n",
      "+-------------------+------------------+---------+----------+---------+---------+---------+---------+---------+-----+\n",
      "|               null|              null|     null|      null|     null|     null|     null|     null|     null| null|\n",
      "|1237662224594305220|588017978346111168|156.06728|  36.04212|20.106056|18.252907|17.334618|16.946459|16.623198|  221|\n",
      "|1237655691937251730|587729970712347008| 215.3721|-2.4439995|20.331465| 18.68993| 17.79132|17.342367|16.958593|   41|\n",
      "+-------------------+------------------+---------+----------+---------+---------+---------+---------+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "photobj.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----+-----+-------+------+\n",
      "|              objid|redshift|plate|  mjd|fiberid| class|\n",
      "+-------------------+--------+-----+-----+-------+------+\n",
      "|               null|    null| null| null|   null|  null|\n",
      "|1237662224594305220|0.088759| 1957|53415|    148|GALAXY|\n",
      "|1237655691937251730|0.127493|  917|52400|    111|GALAXY|\n",
      "+-------------------+--------+-----+-----+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "specobj.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pyspark.sql.dataframe.DataFrame, pyspark.sql.dataframe.DataFrame)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(specobj), type(photobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive.registerDataFrameAsTable(photobj_csv, \"photobj_csv\")\n",
    "hive.registerDataFrameAsTable(photobj, \"photobj\")\n",
    "hive.registerDataFrameAsTable(specobj_csv, \"specobj_csv\")\n",
    "hive.registerDataFrameAsTable(specobj, \"specobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |    photobj|       true|\n",
      "|        |photobj_csv|       true|\n",
      "|        |    specobj|       true|\n",
      "|        |specobj_csv|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+----------+---------+---------+---------+---------+---------+-----+\n",
      "|              objid|            fileid|       ra|       dec|        u|        g|        r|        i|        z|field|\n",
      "+-------------------+------------------+---------+----------+---------+---------+---------+---------+---------+-----+\n",
      "|               null|              null|     null|      null|     null|     null|     null|     null|     null| null|\n",
      "|1237662224594305220|588017978346111168|156.06728|  36.04212|20.106056|18.252907|17.334618|16.946459|16.623198|  221|\n",
      "|1237655691937251730|587729970712347008| 215.3721|-2.4439995|20.331465| 18.68993| 17.79132|17.342367|16.958593|   41|\n",
      "+-------------------+------------------+---------+----------+---------+---------+---------+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT * FROM photobj LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----+-----+-------+------+\n",
      "|              objid|redshift|plate|  mjd|fiberid| class|\n",
      "+-------------------+--------+-----+-----+-------+------+\n",
      "|               null|    null| null| null|   null|  null|\n",
      "|1237662224594305220|0.088759| 1957|53415|    148|GALAXY|\n",
      "|1237655691937251730|0.127493|  917|52400|    111|GALAXY|\n",
      "+-------------------+--------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT * FROM specobj LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   25081|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT COUNT(*) FROM specobj\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar la fila de valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   25080|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT COUNT(*) FROM specobj WHERE objid is not null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y registramos dos tablas nuevas sin valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "photobj_csv = hive.sql(\"SELECT * FROM photobj_csv WHERE objid is not null\")\n",
    "photobj = hive.sql(\"SELECT * FROM photobj WHERE objid is not null\")\n",
    "specobj_csv = hive.sql(\"SELECT * FROM specobj_csv WHERE objid is not null\")\n",
    "specobj = hive.sql(\"SELECT * FROM specobj WHERE objid is not null\")\n",
    "\n",
    "hive.registerDataFrameAsTable(photobj_csv, \"photobj_csv\")\n",
    "hive.registerDataFrameAsTable(photobj, \"photobj\")\n",
    "hive.registerDataFrameAsTable(specobj_csv, \"specobj_csv\")\n",
    "hive.registerDataFrameAsTable(specobj, \"specobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+---+-------+-----+\n",
      "|objid|redshift|plate|mjd|fiberid|class|\n",
      "+-----+--------+-----+---+-------+-----+\n",
      "+-----+--------+-----+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT * FROM specobj WHERE objid is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT objid)|\n",
      "+---------------------+\n",
      "|                25054|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT COUNT(DISTINCT objid) FROM specobj_csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por construcción nuestra tabla `specobj` no contiene datos de estrellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "| class|count(1)|\n",
      "+------+--------+\n",
      "|GALAXY|   25080|\n",
      "|  STAR|      20|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT `class`, COUNT(*) FROM specobj_csv GROUP BY `class`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "| class|count(1)|\n",
      "+------+--------+\n",
      "|GALAXY|   25080|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT `class`, COUNT(*) FROM specobj GROUP BY `class`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por construcción nuestra tabla `photobj` tiene 25060 filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   25060|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT COUNT(*) FROM photobj_csv WHERE u != -9999.0 AND g != -9999.0 AND r != -9999.0 AND i != -9999.0 AND z != -9999.0 LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   25060|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hive.sql(\"SELECT COUNT(*) FROM photobj LIMIT 3\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
