{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data y Machine Learning para clasificación de galaxias\n",
    "\n",
    "- [Conclusión final](#ml)\n",
    "    - [Posibles mejoras](#mejoras)\n",
    "    - [Dificultades y limitaciones](#dificultades)\n",
    "\n",
    "    \n",
    "<div id='xx' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='conslusion' />\n",
    "\n",
    "## Conclusión final\n",
    "\n",
    "Tal como comentamos al principio de este trabajo, nuestra idea inicial era construir un **producto** Big Data que pudiera implementarse fácil y rápidamente, y que a su vez sirviese a científicos e investigadores como plataforma para ampliar el conocimiento sobre el mundo que nos rodea. Con este propósito en mente hemos intentado abarcar los siguientes puntos:\n",
    "\n",
    "1. Introducción teórica sobre los tipos de galaxias y relación con el mundo Big Data.\n",
    "\n",
    "\n",
    "2. Planteamiento del problema y retos que supone la gestión de la basta cantidad de datos disponibles. Descripción de la necesidad de algoritmos y programas automáticos para llevar a cabo la clasificación de las imágenes tomadas por los telescopios.\n",
    "\n",
    "\n",
    "3. Descarga y configuración de las herramientas que utilizamos en la construcción del producto: ecosistema Cloudera (HADDOP, YARN, HDFS, HIVE, HUE, IMPALA, SPARK), contenedores Docker, plataforma de análisis Anaconda, etc.\n",
    "    \n",
    "    \n",
    "4. Descarga y almacenamiento de datos a gran escala.\n",
    "    \n",
    "    4.1 Explicación de cómo llevar a cabo un almacenamiento en HDFS.\n",
    "    \n",
    "    4.2 Explicación de qué es el modelo realacional para bases de datos y cómo hacer un almacenamiento en HIVE.\n",
    "    \n",
    "    \n",
    "5. Carga y visualización de datos. Principalmente mediante HUE para hacer consultas, análisis exploratorio y visualización.\n",
    "    \n",
    "    \n",
    "6. Desarrollo de modelos.\n",
    "\n",
    "    6.1 Reducción de la dimensionalidad mediante PCA.\n",
    "    \n",
    "    6.2 Modelo de clasificación mediante Regresión Logística.\n",
    "    \n",
    "    6.3 Modelo de clasificación mediante una red neuronal Perceptrón Multicapa.\n",
    "    \n",
    "    6.4 Comparación de modelos mediante métricas tales como matriz de confusión y curva ROC.\n",
    "    \n",
    "    \n",
    "7. Otras tareas como pueden ser desarrollo de scripts para descarga de herramientas, datos e imágenes, construcción de datasets y funciones.\n",
    "\n",
    "---\n",
    "\n",
    "<div id='mejoras' />\n",
    "\n",
    "#### Posibles mejoras\n",
    "\n",
    "Cabe destacar que quizás no hemos profundizado lo suficiente en algunos de estos puntos. A este respecto cabe citar las siguientes:\n",
    "\n",
    "- Hacer una amplia explicación del conjunto de herramientas presentes en el ecosistema Cloudera que no han sido mencionadas y que además podrían ser de gran utilidad en un proyecto Big Data de estas características (Solr, Flume, Sqoop, Zeppelin, etc). \n",
    "\n",
    "\n",
    "- También sería posible explicar la configuración para la ingesta de datos en HDFS y HIVE de manera automática mediante Flume y Sqoop, respectivamente.\n",
    "\n",
    "\n",
    "- Presentar un mayor número de gráficas y visualizaciones de los datos almacenados en HIVE (dataset de características y parámetros de galaxias y telescopios).\n",
    "\n",
    "\n",
    "- Comparación con otros modelos basados en deep learninig (por ejemplo una red CNN).\n",
    "\n",
    "\n",
    "A pesar de cierto déficit en la profundidad entorno a estas cuestiones, es menester mencionar que la máxima durante este trabajo consistió en hacer una buena configuración del entorno Cloudera con sus debidas explicaciones; así como de hacer un desarrollo de código lo más extenso posible (HUE-HIVE, SPARK, scripts, modelos, etc).\n",
    "\n",
    "---\n",
    "\n",
    "<div id='dificultades' />\n",
    "\n",
    "#### Dificultades y limitaciones\n",
    "\n",
    "Cuanto más grande sea el proyecto, más dificultades podemos encontrar en el camino. \n",
    "\n",
    "La principal dificultad en el transcurso de este proyecto fue la elección del entorno y su configuración. La premisa es que debíamos trabajar en un ecosistema Big Data (pues de eso trata este máster) y dado que ya hemos visto cómo trabajar en Hortonworks durante el máster, consideramos que sería buena idea conocer su \"hermano gemelo\" Cloudera. \n",
    "\n",
    "Tomada la desición, la siguiente fue sobre qué entorno elegir: ¿máquina virtual, docker, Cloud? La elección de docker se debió a características técnicas de la máquina. A pesar de ello se presentó el problema de la falta de memoria RAM, lo cual supuso un punto de inflexión durante el trabajo, pues implicaba reconsiderar todo el proyecto en sí mismo. En resumen, suponía abandonar el enfoque Data Engineer (o Architect, según se mire) en favor de una vertiente más Data Science para así enfocarnos más en el desarrollo y comparativa de modelos. Pero como dijimos, la intención era hacer un producto Big Data. Es por esto y para seguir trabajando con docker y SPARK por la que nos decantamos por usar el stack de *Docker Jupyter Notebooks* para la parte de modelo.\n",
    "\n",
    "Y por último, el tercer gran inconveniente fue la siguiente pregunta: ¿de donde sacamos las imágenes? El proyecto *GalaxyZoo* ofrece el dataset de clasificaciones en base al *objid* de la imagen (identificador de la imagen) y en formato `.csv`. El único sitio donde recurrir a las imágenes era en la web del proyecto *SDSS*. Sin embargo, para descargar las imágenes en conjunto (*bulk*) solo es posible de las siguientes maneras:\n",
    "\n",
    "- A través de una plataforma mediante consultas SQL sobre la que hay que [registrarse](https://skyserver.sdss.org/CasJobs/). Supone además descargar [software](http://skyserver.sdss.org/dr14/en/help/download/downloadhome.aspx) específico y conocer la estructura de su base de datos.\n",
    "\n",
    "\n",
    "- A través de [enlaces](https://data.sdss.org/datamodel/) y/o [comandos](https://www.sdss.org/dr14/data_access/bulk/) por consola. Lo cual descargaba los datos en formato [`.fits`](https://es.wikipedia.org/wiki/FITS) (típico en el ámbito de la astronomía). Lo cual suponía el trabajo extra de saber manipular este formato de ficheros.\n",
    "\n",
    "\n",
    "- La opción más viable era en esta [web](https://dr12.sdss.org/bulkFields). La cual ofrece la posibilidad de descargar los datos en *bulk*. Sin embargo, aún seleccionando los valores adecuados de `name, ra, dec` no era posible descargar imágenes aceptables con el tamaño adecuado (la calidad de la imagen era notable y de difícil procesamiento para reducirlas a `64x64` píxeles).\n",
    "\n",
    "\n",
    "Finalmente encontramos la manera de hacer [web scraping](https://es.wikipedia.org/wiki/Web_scraping) en la siguiente [web](https://skyserver.sdss.org/dr14/en/tools/chart/listinfo.aspx) del proyecto *SDSS* para la descarga automática."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
